{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ced2ae8d"
   },
   "source": [
    "# CS4243 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10720a21"
   },
   "source": [
    "Deep learning in computer vision is typically targeted at tasks that humans excel in, but computers find much harder to process. Nowadays, with the advent of numerous deep learning architectures like VGG19, object recognition and segmentation tasks are well within the realms of reasonable accuracy for computers. In this project, we target a task that even humans might struggle with - the classification of paintings. While a layperson might be able to distinguish the brushy, impressionist style of Auguste Renoir from the distinctive abstract expressionist style of Jackson Pollock, a strong background in art is often necessary for the more nuanced painting classifications.\n",
    "\n",
    "The objective of this project is to train a deep learning model to classify paintings according to various attributes (artist, genre, style) and investigate what features allow certain paintings to be grouped together distinctly from others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNxCqyYHmWe4"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rREQIQ_wl7Cg"
   },
   "source": [
    "*Note: Due to limitations in training and testing, this report will be focused on the artist attribute over the other attributes where necessary. We also limit ourselves to four classes in each category for ease of training.*\n",
    "\n",
    "The data and other files referred to in this notebook can be found [here](https://drive.google.com/drive/folders/1HswCm4dw4AOHlGv0fkceXiMx_TwTlBJk?usp=sharing). This notebook was created using a variety of platforms including Google Colab, Kaggle, and the author's personal computer. While care has been taken to ensure cross-compatability between platforms, sections of code might need slight modifications to be suitable for a specific environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6da363e5"
   },
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0148e616"
   },
   "source": [
    "### Selecting the artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ecf923c"
   },
   "source": [
    "The first step of data acquisition is to select the artists we wish to investigate. Artists with a large amount of published work would be preferred so as to increase the amount of training data available to our model. We also wish to select painters who are well-known. \n",
    "\n",
    "For this data, we use `BeautifulSoup` and `Selenium` to scrape a page on [WikiART](https://www.wikiart.org/en/popular-artists) - an online visual art encyclopedia - that lists 240 of its most popular artists by most visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6f128aa"
   },
   "source": [
    "The code for scraping the webpage:\n",
    "\n",
    "*Note: It requires an appropriate version of chromedriver to be installed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e6ac55a"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from operator import itemgetter\n",
    "import time\n",
    "import csv\n",
    "\n",
    "URL = \"https://www.wikiart.org/en/Popular-Artists/\" \n",
    "SLEEP_TIME = 3\n",
    "NUM_OF_ARTISTS = 4\n",
    "\n",
    "options = Options()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "# change this path as necessary\n",
    "# correct version of chromedriver must be installed\n",
    "service = Service(\"C:\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(URL) \n",
    "\n",
    "# load time\n",
    "time.sleep(SLEEP_TIME) \n",
    "\n",
    "# close pop-up if it exists\n",
    "try:\n",
    "    driver.find_element(By.XPATH, \"//button[contains(@class, 'needsclick')]\").click()\n",
    "    time.sleep(SLEEP_TIME) \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# scroll to bottom\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "load_button = driver.find_element(By.XPATH, \"//a[contains(.,'LOAD MORE')]\")\n",
    "\n",
    "for _ in range(SLEEP_TIME):\n",
    "    load_button.click()\n",
    "    time.sleep(SLEEP_TIME) \n",
    "\n",
    "    # scroll to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(SLEEP_TIME) \n",
    "\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//button[contains(@class, 'needsclick')]\").click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "data = soup.find(\"ul\", class_=\"wiki-artistgallery-container\")\n",
    "\n",
    "# parse artist names and work count\n",
    "artist_names = []\n",
    "work_count = []\n",
    "for li in data.find_all(\"li\"):\n",
    "    artist_name = li.find(\"div\", class_=\"artist-name\").find(\"a\")\n",
    "    artist_names.append(artist_name.text.strip())\n",
    "    num_of_works = li.find(\"div\", class_=\"works-count\")\n",
    "    try:\n",
    "        work_count.append(int(num_of_works.text.strip().split()[0]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# find top artists\n",
    "top_artists = []\n",
    "for artist, num_of_works in zip(artist_names, work_count):\n",
    "    if len(top_artists) < NUM_OF_ARTISTS:\n",
    "        top_artists.append([artist, num_of_works])\n",
    "        top_artists = sorted(top_artists, key=itemgetter(1))\n",
    "    elif num_of_works > top_artists[0][1]:\n",
    "        top_artists.pop(0) \n",
    "        top_artists.append([artist, num_of_works])\n",
    "        top_artists = sorted(top_artists, key=itemgetter(1))\n",
    "\n",
    "print(top_artists)\n",
    "\n",
    "# export to csv\n",
    "with open(\"most_popular_classical_artists.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(top_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d71dd8c5"
   },
   "source": [
    "At the time of writing, the four popular artists with the most number of works are\n",
    "1. Vincent van Gogh, 1931 works\n",
    "2. Nicholas Roerich, 1843 works\n",
    "3. Pierre-Auguste Renoir, 1412 works\n",
    "4. Claude Monet, 1366 works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77dcedba"
   },
   "source": [
    "### Obtaining information about artworks by the artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ad6e427"
   },
   "source": [
    "The next step is to obtain the list of works by each artist. We do this by querying the [WikiART JSON API](https://www.wikiart.org/en/App/GetApi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfe31b49"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "path = globals()['_dh'][0] + \"/\"\n",
    "df = pd.read_csv(path + '\\most_popular_classical_artists.csv', sep=',', names=[\"artist_names\", \"num_of_works\"])\n",
    "print(df)\n",
    "artists = df[\"artist_names\"].str.lower()\n",
    "artists = artists.replace(' ', '-', regex=True)\n",
    "json_urls = artists.map('https://www.wikiart.org/en/App/Painting/PaintingsByArtist?artistUrl={}&json=2'.format)\n",
    "\n",
    "for url, artist in zip(json_urls, artists):\n",
    "    page = requests.get(url)\n",
    "    artworks = json.loads(page.text)\n",
    "    df = pd.DataFrame(artworks)\n",
    "    df.to_csv(path + '\\dataframe_of_artworks\\\\' + artist + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f308396"
   },
   "source": [
    "### Downloading images of artworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e5666a8"
   },
   "source": [
    "A `csv` file of the list of artworks and their relevant information in JSON format are thus obtained for each artist. The files contain links to view the respective artworks as well, which we utilize to download `png` images of the artworks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecc38553"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "path = globals()['_dh'][0] + \"/\"\n",
    "dir_path = path + '\\dataframe_of_artworks\\\\'\n",
    "files = os.listdir(dir_path)\n",
    "\n",
    "filepath = Path(path).parent.parent.absolute()\n",
    "filepath = str(filepath) + '\\\\data\\\\classical_art\\\\'\n",
    "\n",
    "for file in files:\n",
    "    # precautionary measure\n",
    "    if not os.path.isfile(os.path.join(dir_path, file)):\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(dir_path + file, sep=',')\n",
    "    image_urls = df[\"image\"]\n",
    "    filenames = df[\"contentId\"].map(\"{}.jpg\".format)\n",
    "\n",
    "    # adapted from https://stackoverflow.com/questions/30229231/python-save-image-from-url#answer-30229298 by DeepSpace\n",
    "    for url, filename in zip(image_urls, filenames):\n",
    "        # skip if file already exists\n",
    "        if os.path.isfile(os.path.join(filepath, filename)):\n",
    "            continue\n",
    "\n",
    "        response = requests.get(url, stream = True)\n",
    "\n",
    "        if not response.ok:\n",
    "            print(\"Image with content id\", filename, \"could not be downloaded successfully.\")\n",
    "            continue\n",
    "\n",
    "        with open(filepath + filename, 'wb') as f:\n",
    "            for block in response.iter_content(1024):\n",
    "                if not block:\n",
    "                    break\n",
    "\n",
    "                f.write(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ed2fd6"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSu2opAklHYo"
   },
   "source": [
    "Run the following code to be able to view the diagrams later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2251,
     "status": "ok",
     "timestamp": 1651018229265,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "Erk9WOjclLQZ",
    "outputId": "8430c237-bd3a-42c9-80c6-a05cdae33dbb"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_Project/'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "\n",
    "current_folder = globals()['_dh'][0] + \"/\"\n",
    "try:\n",
    "  current_folder = path_to_file\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "620f3399"
   },
   "source": [
    "### Spread of data across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b35cf422"
   },
   "source": [
    "We first look at the number of artworks across classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1650988540239,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "f87f22dd",
    "outputId": "f482d9d1-19c6-4b0d-8ddc-209516869fb1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "path = current_folder\n",
    "df = pd.read_csv(path + 'downloaded_data.csv', sep=',')\n",
    "\n",
    "# num of artworks per artists\n",
    "num_of_artwork_artists = df['artistName'].value_counts()\n",
    "label = [\"Vincent van Gogh\", \"Pierre-Auguste Renoir\", \"Monet Claude\", \"Roerich Nicholas\"]\n",
    "\n",
    "num_of_artwork_artists.plot.pie(figsize=(10, 10), autopct='%1.1f%%', labels=label, label=\"\")\n",
    "print(num_of_artwork_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cb10daf"
   },
   "source": [
    "Observe that the dataset is slightly unbalanced with respect to artists at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0db0354"
   },
   "source": [
    "Notice that there are some rows with multiple entries for genre and style. Here, `value_counts` does not help much - we calculate the frequency of words instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1650988545923,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "a40a2634",
    "outputId": "3bbb1684-f811-421d-bfac-19e05da1db5d"
   },
   "outputs": [],
   "source": [
    "# num of artworks per genre\n",
    "num_of_artwork_genre = df['genre'].str.get_dummies(sep=', ').sum()\n",
    "df_genre_count = pd.DataFrame({'genre':num_of_artwork_genre.index, 'count':num_of_artwork_genre.values})\n",
    "df_genre_count = df_genre_count.sort_values(by=['count'])\n",
    "\n",
    "plt = df_genre_count.plot.pie(y='count', figsize=(10, 10), autopct='%1.1f%%', labels=df_genre_count['genre'], label=\"\")\n",
    "plt.legend(bbox_to_anchor=(1.25, 1), loc='upper left', borderaxespad=0)\n",
    "print(df_genre_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2650faa2"
   },
   "source": [
    "There are many genres with negligible number of artwork. `Poster`, for instance, has a singular associated artwork. Oversampling them would thus be unnecessary; they can later be removed from the artwork without significant loss on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1650988550987,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "f84a160a",
    "outputId": "9f54d10a-64ee-4ec8-f05a-d0c98d101d1c"
   },
   "outputs": [],
   "source": [
    "# num of artworks per style\n",
    "num_of_artwork_style = df['style'].str.get_dummies(sep=', ').sum()\n",
    "df_style_count = pd.DataFrame({'style':num_of_artwork_style.index, 'count':num_of_artwork_style.values})\n",
    "df_style_count = df_style_count.sort_values(by=['count'])\n",
    "\n",
    "plt = df_style_count.plot.pie(y='count', figsize=(10, 10), autopct='%1.1f%%', labels=df_style_count['style'], label=\"\")\n",
    "plt.legend(bbox_to_anchor=(1.25, 1), loc='upper left', borderaxespad=0)\n",
    "print(df_style_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15da0072"
   },
   "source": [
    "As with genre, there are many styles with negligible number of artwork as well. `Academicism`, for instance, has a singular associated artwork. Oversampling these would be unnecessary; they can later be removed from the artwork without significant loss on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "711c66a2"
   },
   "source": [
    "### Relationships between attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bac59ec"
   },
   "source": [
    "We next take a look at whether there are correlations between different attributes.\n",
    "\n",
    "Plotting a scatter plot for the dimensions of an artwork against their artist shows a correlation between small dimensions and Roerich Nicholas. That is, if a given artwork is large, it is likely to not be attributable to Roerich Nicholas. \n",
    "\n",
    "This holds important implications for the data pre-processing stage that will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1926,
     "status": "ok",
     "timestamp": 1650988559600,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "517fedb3",
    "outputId": "7eeb9e18-b0f9-4fef-d88d-c4cc2e60115e"
   },
   "outputs": [],
   "source": [
    "# width, height vs artist \n",
    "widthHeightArtistPlot = px.scatter(df, x=\"width\", y=\"height\", color=\"artistName\", color_discrete_map={\n",
    "                \"van Gogh Vincent\": \"red\",\n",
    "                \"Renoir Pierre-Auguste\": \"green\",\n",
    "                \"Monet Claude\": \"blue\",\n",
    "                \"Roerich Nicholas\": \"gray\"}, custom_data=['genre', 'style', 'title', 'completitionYear'],\n",
    "                labels={\n",
    "                \"artistName\": \"Artist\",  \"width\": \"Width of painting\", \"height\": \"Height of painting\"\n",
    "            })\n",
    "\n",
    "widthHeightArtistPlot.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"Title: %{customdata[2]}\",\n",
    "        \"Genre: %{customdata[0]}\",\n",
    "        \"Style: %{customdata[1]}\",\n",
    "        \"Width: %{x}\",\n",
    "        \"Height: %{y}\",\n",
    "        \"Completion year: %{customdata[3]}\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "widthHeightArtistPlot.update_layout(\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "widthHeightArtistPlot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18a22c9e"
   },
   "source": [
    "Plotting a scatter plot for the dimensions of an artwork against the year of completion, on the other hand, shows a clear correlation between later years and a slow progression towards smaller dimensions. This might be in part due to the ongoing WW2 efforts in the early 1940s.\n",
    "\n",
    "*Note: The year of completion of an artwork is not covered in this project, but there is potential for future work on this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1650988566753,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "ef172561",
    "outputId": "5a2c795b-8a17-42e2-cc09-0ebd9a0041d2"
   },
   "outputs": [],
   "source": [
    "# width, height vs completionYear\n",
    "widthHeightYearPlot = px.scatter(df, x=\"width\", y=\"height\", color=\"completitionYear\", \\\n",
    "        custom_data=['genre', 'style', 'title', 'artistName'], \\\n",
    "        labels={\"completitionYear\": \"Completion year\",  \"width\": \"Width of painting\", \"height\": \"Height of painting\"})\n",
    "\n",
    "widthHeightYearPlot.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"Title: %{customdata[2]}\",\n",
    "        \"Artist name: %{customdata[3]}\",\n",
    "        \"Genre: %{customdata[0]}\",\n",
    "        \"Style: %{customdata[1]}\",\n",
    "        \"Width: %{x}\",\n",
    "        \"Height: %{y}\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "widthHeightYearPlot.update_layout(\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "widthHeightYearPlot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ad66ec"
   },
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72f959db"
   },
   "source": [
    "### Handling data points with multiple entries in genre, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49749dba"
   },
   "source": [
    "As mentioned in the data exploration stage, there are a few data points with multiple entries for genre and style in the pandas dataframe and handling this appropriately is essential before moving onto additional pre-processing steps.\n",
    "\n",
    "There are three main approaches:\n",
    "1. Select one of the entries for each row\n",
    "2. Add duplicate rows, with each row having a singular valid entry in the respective column\n",
    "3. Remove rows from dataset\n",
    "\n",
    "**Approach 1**\n",
    "\n",
    "Selecting one entry for each such data point, in essence, means rejecting the other entry for that data point. This might contribute to building up inaccuracies in the model.\n",
    "\n",
    "**Approach 2**\n",
    "\n",
    "This is likely to yield a more accurate model than approach 1, since each entry is represented once, and there is more data. However, this might make testing more complicated, for a singular image can have only one predicted output label for a given attribute in typical architectures. \n",
    "\n",
    "**Approach 3**\n",
    "\n",
    "These entries do not make up a substantial part of the dataset, and thus can be safely discarded. This was the chosen approach. The code for this is given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4289b2b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "path = current_folder\n",
    "df = pd.read_csv(path + 'downloaded_data.csv', sep=',')\n",
    "\n",
    "df = df[~df['genre'].str.contains(\",\")]\n",
    "num_of_artwork_genre = df['genre'].value_counts()\n",
    "\n",
    "df = df[~df['style'].str.contains(\",\")]\n",
    "num_of_artwork_style = df['style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa3c2b63"
   },
   "source": [
    "### Filtering the top four genre, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38c42feb"
   },
   "source": [
    "The next step is to filter by genre and style, thereby removing small classes from the dataset. We do this by sorting the top four genre and style with most artworks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1651018273071,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "207725c0",
    "outputId": "5d0c9300-d775-4088-8c99-4d3c957271bb"
   },
   "outputs": [],
   "source": [
    "# select top 4 genre and style\n",
    "selected_genre = num_of_artwork_genre.index[:4].tolist()\n",
    "selected_style = num_of_artwork_style.index[:4].tolist()\n",
    "print(selected_genre)\n",
    "print(selected_style)\n",
    "\n",
    "df = df[df['genre'].isin(selected_genre)]\n",
    "df = df[df['style'].isin(selected_style)]\n",
    "\n",
    "df.to_csv(path + '\\\\cleaned_downloaded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36bbefa3"
   },
   "source": [
    "We then analyze the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1651018279565,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "2faddb4b",
    "outputId": "1a3bb89c-f67e-446f-f837-939707655764"
   },
   "outputs": [],
   "source": [
    "# num of artworks per artists\n",
    "num_of_artwork_artists = df['artistName'].value_counts()\n",
    "label = [\"Vincent van Gogh\", \"Pierre-Auguste Renoir\", \"Monet Claude\", \"Roerich Nicholas\"]\n",
    "plt2 = num_of_artwork_artists.plot.pie(figsize=(10, 10), autopct='%1.1f%%', labels=label, label=\"\")\n",
    "print(num_of_artwork_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1651018283760,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "4da1713c",
    "outputId": "58bd44c7-b404-4356-c8f1-11bccc62dc64"
   },
   "outputs": [],
   "source": [
    "# num of artworks per genre\n",
    "num_of_artwork_genre = df['genre'].value_counts()\n",
    "plt3 = num_of_artwork_genre.plot.pie(figsize=(10, 10), autopct='%1.1f%%', label=\"\")\n",
    "print(num_of_artwork_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1651018287840,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "2476f6ab",
    "outputId": "100ec1b2-631c-4279-fe7d-516072271c28"
   },
   "outputs": [],
   "source": [
    "# num of artworks per style\n",
    "num_of_artwork_style = df['style'].value_counts()\n",
    "pl4 = num_of_artwork_style.plot.pie(figsize=(10, 10), autopct='%1.1f%%', label=\"\")\n",
    "print(num_of_artwork_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1651018289840,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "80fae778",
    "outputId": "4b45e024-fccf-466c-b89a-6b340eec1ebd"
   },
   "outputs": [],
   "source": [
    "grouped_df = df[[\"genre\", \"style\", \"artistName\"]]\n",
    "grouped_df = grouped_df.groupby(grouped_df.columns.tolist(),as_index=False).size()\n",
    "\n",
    "fig = px.scatter(grouped_df, x=\"genre\", y=\"style\", size=\"size\", color=\"artistName\", \\\n",
    "                 hover_name=\"artistName\", size_max=60, \\\n",
    "                 labels = {\"genre\":\"Genre\", \"style\":\"Style\", \"size\":\"Num of works\", \"artistName\":\"Artist\"}, \\\n",
    "                 custom_data=[\"size\", \"artistName\"])\n",
    "\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"Genre: %{x}\",\n",
    "        \"Style: %{y}\",\n",
    "        \"Num of works: %{customdata[0]}\",\n",
    "        \"Artist: %{customdata[1]}\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(current_folder + \"augmented_data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25f3a657"
   },
   "source": [
    "At this point, the data is rather unbalanced for style, genre and artist attributes as can be seen from the bubble chart, as well as from the pie charts. A crucial point of observation is that certain artists have strong correlation with certain styles. Roerich Nicholas's works, for instance, appears to lean mostly into symbolism, whereas Vincent van Gogh has many artworks under post-impressionism and realism, but little to none for impressionism and symbolism. \n",
    "\n",
    "This is expected - Vincent van Gogh, for example, is considered to be one of the most influential post-impressionist painters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51123e64"
   },
   "source": [
    "### Add padding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ac674c6"
   },
   "source": [
    "The next step in pre-processing is to ensure that all images are of the same size. There are multiple ways to achieve this. One can transform larger images to become smaller ones by means of cropping, stretching, etc. Smaller images can also be transformed to become larger images by means of stretching, padding etc. \n",
    "\n",
    "As observed in the earlier data exploration stage, the dimensions (width and height) of an artwork might provide meaningful information to a model about an artwork being the handiwork of certain artists. As such, maintaining this feature is essential in this step of post-processing. Any sort of affine transformation or stretching does not preserve the relative sizes of any two given images. In addition, affine transformations might distort artistic traits of paintings as well.\n",
    "\n",
    "Therefore, padding is chosen to resize images to a uniform size. All images are padded to the largest dimensions any image in the dataset has - in this case, 600 (H) x 750 (W). The code for that is as follows.\n",
    "\n",
    "*Warning: Do not run the following cell unless you wish to overwrite data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdb9f686"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "path = globals()['_dh'][0] + \"/\"\n",
    "df = pd.read_csv(path + '\\cleaned_downloaded_data.csv', sep=',')\n",
    "\n",
    "image_path = str(path) + \"\\\\data\\\\classical_art\\\\\"\n",
    "image_names = df[\"contentId\"].map(\"{}.jpg\".format)\n",
    "\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "for image_name in image_names:\n",
    "    # if file does not exist\n",
    "    filepath = os.path.join(image_path, image_name)\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"Cannot find image file with content id \", image_name)\n",
    "        continue\n",
    "\n",
    "    img=cv2.imread(filepath)\n",
    "    height, width = img.shape[0:2]\n",
    "    if width > max_width:\n",
    "        max_width = width\n",
    "    if height > max_height:\n",
    "        max_height = height\n",
    "\n",
    "print(max_height, max_width)\n",
    "\n",
    "# add padding\n",
    "for image_name in image_names:\n",
    "    # if file does not exist\n",
    "    filepath = os.path.join(image_path, image_name)\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"Cannot find image file with content id \", image_name)\n",
    "        continue\n",
    "    \n",
    "    img = cv2.imread(filepath)\n",
    "    height, width = img.shape[0:2]\n",
    "\n",
    "    top_padding = math.floor((max_height - height) / 2)\n",
    "    bottom_padding = max_height - height - top_padding\n",
    "    left_padding = math.floor((max_width - width) / 2)\n",
    "    right_padding = max_width - width - left_padding\n",
    "    image = cv2.copyMakeBorder(img, top_padding, bottom_padding, left_padding, right_padding, \\\n",
    "        cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "\n",
    "    new_name = \"_\" + image_name\n",
    "    new_filepath = os.path.join(str(path) + \"\\\\data\\\\padded_images\\\\\", new_name)\n",
    "    cv2.imwrite(new_filepath, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd680e30"
   },
   "source": [
    "### Augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fd36b54"
   },
   "source": [
    "The focus of this project will be on classifing the artist of artworks. Given the difficult task of normalising data across three different attributes, each with four different classes, the goal of this step will be to normalize data with respect to the artist classes. Each artist will have ~30k images. For this, van Gogh's artworks have to be augmented once, Renoir's artwork twice, Monet three times, and Roerich seven times.\n",
    "\n",
    "In line with minimizing distortions on the artwork, only transformations that preserve the local artwork structure will be used. In addition, to ensure the heterogenity of images, the transformations applied will be random. In this case, they are \n",
    "- RandomHorizontalFlip\n",
    "- RandomResizedCrop\n",
    "- RandomVerticalFlip.\n",
    "\n",
    "The code for this is as follows.\n",
    "\n",
    "*Note: Do not run the following cell unless you wish to overwrite data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9e5bc7b"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "path = globals()['_dh'][0] + \"/\"\n",
    "df = pd.read_csv(path + '\\cleaned_downloaded_data.csv', sep=',')\n",
    "\n",
    "# roughly normalize dataset \n",
    "artists = df[\"artistName\"].unique()\n",
    "#print(dict)\n",
    "#['van Gogh Vincent ' 'Monet Claude' 'Renoir Pierre-Auguste' 'Roerich Nicholas']\n",
    "counts = [1, 3, 2, 7]\n",
    "artist_multiplier = dict(zip(artists, counts))\n",
    "\n",
    "image_path = str(path) + \"\\\\data\\\\padded_images\\\\\"\n",
    "image_names = df[\"contentId\"].map(\"_{}.jpg\".format)\n",
    "\n",
    "max_height = 600\n",
    "max_width = 750\n",
    "aspect_ratio = max_height / max_width\n",
    "\n",
    "T = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop((max_height, max_width), scale=(0, 1), ratio=(aspect_ratio,aspect_ratio))\n",
    "])\n",
    "\n",
    "for image_name, artist in zip(image_names, df[\"artistName\"]):\n",
    "    # if file does not exist\n",
    "    filepath = os.path.join(image_path, image_name)\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"Cannot find image file with content id \", image_name)\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(filepath)\n",
    "\n",
    "    for j in range(artist_multiplier[artist]):\n",
    "        # apply transformation, save as new image\n",
    "        image = T(img)\n",
    "\n",
    "        new_name = \"_\" + str(j) + image_name\n",
    "        new_filepath = os.path.join(str(path) + \"\\\\data\\\\padded_images\\\\\", new_name)\n",
    "        torchvision.utils.save_image(image, new_filepath)\n",
    "\n",
    "df_updated = df.copy(deep=True)\n",
    "df_updated['contentId'] = '_' + df_updated['contentId'].astype(str) \n",
    "print(df_updated['contentId'].head())\n",
    "\n",
    "for artist in artists:\n",
    "    multiplier = artist_multiplier[artist]\n",
    "    for i in range(multiplier):\n",
    "        temp_df = df[df[\"artistName\"] == artist]\n",
    "        temp_df[\"contentId\"] = \"_\" + str(i) + \"_\" + temp_df[\"contentId\"].astype(str)\n",
    "        df_updated = pd.concat([df_updated, temp_df])\n",
    "\n",
    "df_updated.to_csv(path + '\\\\augmented_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70edd643"
   },
   "source": [
    "### Reduce size of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5b5374a"
   },
   "source": [
    "The final step of data pre-processing is to reduce the size of the images. While a higher resolution generally tends to yield better results, the current dataset is a little over a gigabyte is size, and loading or otherwise manipulating data is a long process. \n",
    "\n",
    "The size of all images will be reduced from (600,750) to (120,150) using the [Python Imaging Library](https://pillow.readthedocs.io/en/stable/) as follows.\n",
    "\n",
    "*Note: Do not attempt to run the following cell unless you wish to overwrite data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ecbe11b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "path = os.path.dirname(os.path.realpath(__file__))\n",
    "df = pd.read_csv(path + '\\\\augmented_data.csv', sep=',')\n",
    "\n",
    "image_path = str(path) + \"\\\\data\\\\padded_images\\\\\"\n",
    "image_names = df[\"contentId\"].map(\"{}.jpg\".format)\n",
    "\n",
    "max_height = 600\n",
    "max_width = 750\n",
    "divisor = 5\n",
    "\n",
    "for image_name in image_names:\n",
    "    # if file does not exist\n",
    "    filepath = os.path.join(image_path, image_name)\n",
    "    if not os.path.isfile(filepath):\n",
    "        print(\"Cannot find image file with content id \", image_name)\n",
    "        continue\n",
    "    \n",
    "    img = Image.open(filepath)\n",
    "\n",
    "    new_width  = int(max_width / divisor)\n",
    "    new_height = int(max_height / divisor)\n",
    "    img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    new_filepath = os.path.join(str(path) + \"\\\\data\\\\smaller_images\\\\\", image_name)\n",
    "    img.save(new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27680147"
   },
   "source": [
    "That completes the pre-processing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54c2cfc2"
   },
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08af6e70"
   },
   "source": [
    "We first implement a rudimentary multilayer perceptron (MLP) and a vanilla convolutional neural network (CNN) in a 1 vs REST process. Here, we build separate models for each attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46e3054d"
   },
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:09.519010Z",
     "iopub.status.busy": "2022-04-27T12:10:09.518733Z",
     "iopub.status.idle": "2022-04-27T12:10:11.680390Z",
     "shell.execute_reply": "2022-04-27T12:10:11.679258Z",
     "shell.execute_reply.started": "2022-04-27T12:10:09.518979Z"
    },
    "id": "2bDMlAjepNvY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "print('cuda available with GPU:',torch.cuda.get_device_name(0))\n",
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:13.361227Z",
     "iopub.status.busy": "2022-04-27T12:10:13.360934Z",
     "iopub.status.idle": "2022-04-27T12:10:13.369137Z",
     "shell.execute_reply": "2022-04-27T12:10:13.368409Z",
     "shell.execute_reply.started": "2022-04-27T12:10:13.361196Z"
    },
    "id": "9ab33278"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_Project/'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:17.042132Z",
     "iopub.status.busy": "2022-04-27T12:10:17.041725Z",
     "iopub.status.idle": "2022-04-27T12:10:17.050537Z",
     "shell.execute_reply": "2022-04-27T12:10:17.048672Z",
     "shell.execute_reply.started": "2022-04-27T12:10:17.042083Z"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1651025025837,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "FhLriU19pWGz",
    "outputId": "f336a32e-c7e0-42ab-f688-42aa01d74d91"
   },
   "outputs": [],
   "source": [
    "# GPU Setup\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available with GPU:',torch.cuda.get_device_name(0))\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:19.797178Z",
     "iopub.status.busy": "2022-04-27T12:10:19.796569Z",
     "iopub.status.idle": "2022-04-27T12:10:20.718614Z",
     "shell.execute_reply": "2022-04-27T12:10:20.717809Z",
     "shell.execute_reply.started": "2022-04-27T12:10:19.797134Z"
    },
    "id": "bd413b21"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "324c769a"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:25.090610Z",
     "iopub.status.busy": "2022-04-27T12:10:25.090235Z",
     "iopub.status.idle": "2022-04-27T12:10:25.156552Z",
     "shell.execute_reply": "2022-04-27T12:10:25.155800Z",
     "shell.execute_reply.started": "2022-04-27T12:10:25.090558Z"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1651025030837,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "9e382cf9",
    "outputId": "bedea9db-f718-4c6e-cf69-eebd2752079c"
   },
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0] + \"/\"\n",
    "try:\n",
    "  current_folder = path_to_file\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# if environment is Kaggle\n",
    "# change directory path as necessary \n",
    "#current_folder = '../input/exploring-artworks/'\n",
    "\n",
    "#images_location = os.path.join(current_folder,'data/smaller_images/')\n",
    "df = pd.read_csv(current_folder + 'augmented_data.csv', sep=',')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:31.393377Z",
     "iopub.status.busy": "2022-04-27T12:10:31.393090Z",
     "iopub.status.idle": "2022-04-27T12:10:31.401152Z",
     "shell.execute_reply": "2022-04-27T12:10:31.400352Z",
     "shell.execute_reply.started": "2022-04-27T12:10:31.393349Z"
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1651025034982,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "fac235fb",
    "outputId": "70c5ec69-b44b-4f66-ac63-371eae557cac"
   },
   "outputs": [],
   "source": [
    "X = df[\"contentId\"]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae5988bd"
   },
   "source": [
    "#### Size of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1651025036933,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "df531d9a",
    "outputId": "e29d9cae-23b7-4678-da9f-b5129bf2f529"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "size = None\n",
    "image_path = str(current_folder) + \"data/smaller_images/\" + str(X[0]) + \".jpg\"\n",
    "\n",
    "if not os.path.isfile(image_path):\n",
    "    print(\"Cannot find image file with content id \", X[0])\n",
    "else:\n",
    "    img = Image.open(image_path)\n",
    "    display(img)\n",
    "    image = transform(img)\n",
    "    size = image.shape\n",
    "\n",
    "print(size) \n",
    "# Notice torch.Size() = C, H, W\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b13b595"
   },
   "source": [
    "### Save all images as tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1957ef4e"
   },
   "source": [
    "We load each image into a tensor and concatenate all images along a new dimension to obtain a new tensor of all image data.In particular, we seek to avoid the notion of `increasing_list = torch.cat(increasing_list, new_list)` for large lists as this is of quadratic complexity.  \n",
    "\n",
    "We thus progressively stack every 400 consecutive tensors together (empirically determined to be a good cut-off before `torch.cat` operations become significantly slower), before merging the resulting stacks to form a larger tensor later on. \n",
    "\n",
    "*Note: You do not have to run the following cells. The data has been saved into a `.pt` file for later reference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61c237aa",
    "outputId": "d766fd25-3a10-477c-f095-cbd266e96de3"
   },
   "outputs": [],
   "source": [
    "X_data_list = []\n",
    "X_data = torch.empty((0,size[0], size[1], size[2]))\n",
    "\n",
    "i = 0\n",
    "for x in X:\n",
    "    i += 1\n",
    "\n",
    "    image_path = str(current_folder) + \"data/smaller_images/\" + str(x) + \".jpg\"\n",
    "    \n",
    "    # if no file found\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(\"Cannot find image file with content id \", x)\n",
    "        break\n",
    "        continue\n",
    "\n",
    "    image = transform(Image.open(image_path))\n",
    "    if len(X_data.shape) > 3:\n",
    "      # if not first iteration\n",
    "      X_data = torch.cat((X_data, image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))), 0)\n",
    "    else:\n",
    "      # if first iteration\n",
    "      X_data = torch.stack((X_data, image))\n",
    "\n",
    "    if i == 400:\n",
    "        X_data_list.append(X_data)\n",
    "        X_data = torch.empty((0,size[0], size[1], size[2]))\n",
    "        i = 0\n",
    "\n",
    "    print(i, X_data.shape)\n",
    "\n",
    "X_data_list.append(X_data)\n",
    "X_data = torch.empty(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0af0b35",
    "outputId": "0b9c216d-ab1b-4c87-d3c3-5f049a0b526b"
   },
   "outputs": [],
   "source": [
    "X_data = torch.empty((0,size[0], size[1], size[2]))\n",
    "\n",
    "for data in X_data_list:\n",
    "    if X_data.nelement() == 0:\n",
    "        if len(data.shape) == 3:\n",
    "            X_data = data.reshape((1, data.shape[0], data.shape[1], data.shape[2]))\n",
    "        else:\n",
    "            X_data = data\n",
    "    elif len(data.shape) == 3:\n",
    "        X_data = torch.cat([X_data, data.reshape((1, data.shape[0], data.shape[1], data.shape[2]))], dim=0)\n",
    "    else:\n",
    "        X_data = torch.cat([X_data, data], dim=0)\n",
    "    \n",
    "    print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5b9a21e"
   },
   "outputs": [],
   "source": [
    "torch.save(X_data, 'images_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6ffc612"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91a06496"
   },
   "source": [
    "### Split into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21ed5d95"
   },
   "source": [
    "The objective of this section is to train an MLP with respect to artist, genre and style. \n",
    "\n",
    "We first load the image data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:10:36.934458Z",
     "iopub.status.busy": "2022-04-27T12:10:36.934102Z",
     "iopub.status.idle": "2022-04-27T12:10:54.202941Z",
     "shell.execute_reply": "2022-04-27T12:10:54.202189Z",
     "shell.execute_reply.started": "2022-04-27T12:10:36.934420Z"
    },
    "executionInfo": {
     "elapsed": 25843,
     "status": "ok",
     "timestamp": 1651025069061,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "6add2048",
    "outputId": "de84003c-3469-454a-ac02-f6bfa88ec62a"
   },
   "outputs": [],
   "source": [
    "X = torch.load(current_folder+'images_data.pt')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7abb1d0a"
   },
   "source": [
    "Next, we split the data into training sets and testing sets for each attribute. For each attribute, we relabel the classes with numbers. Note that each `X_train_attribute` is different.\n",
    "\n",
    "We use a ratio of 9:1 for training data to testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e434e938"
   },
   "source": [
    "#### Image vs Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:17:48.649903Z",
     "iopub.status.busy": "2022-04-27T11:17:48.649359Z",
     "iopub.status.idle": "2022-04-27T11:17:48.659984Z",
     "shell.execute_reply": "2022-04-27T11:17:48.659280Z",
     "shell.execute_reply.started": "2022-04-27T11:17:48.649844Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1651025069062,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "3e8d55b8",
    "outputId": "8831d25b-37bf-41c5-ae19-065cc4e70973"
   },
   "outputs": [],
   "source": [
    "y_genre = df[\"genre\"]\n",
    "print(y_genre.unique())\n",
    "print(y_genre.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6031c4db"
   },
   "source": [
    "The labels used for genre are as follows:\n",
    "\n",
    "| Genre | Label|\n",
    "| :-- | :-: |\n",
    "| sketch and study | 0 |\n",
    "| portrait | 1 |\n",
    "| genre painting | 2 |\n",
    "| landscape | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:17:50.868054Z",
     "iopub.status.busy": "2022-04-27T11:17:50.867372Z",
     "iopub.status.idle": "2022-04-27T11:17:50.891119Z",
     "shell.execute_reply": "2022-04-27T11:17:50.890439Z",
     "shell.execute_reply.started": "2022-04-27T11:17:50.868019Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1651025069062,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "99b9ad60",
    "outputId": "4380bf3b-f6dd-4e11-862a-1b7877a567aa"
   },
   "outputs": [],
   "source": [
    "labels_genre = {}\n",
    "\n",
    "label = 0\n",
    "for genre in y_genre.unique():\n",
    "    labels_genre[genre] = label\n",
    "    label += 1\n",
    "    \n",
    "for genre, label in labels_genre.items():\n",
    "    y_genre = y_genre.replace({genre:label})\n",
    "\n",
    "print(y_genre.unique())\n",
    "print(y_genre.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:17:52.887702Z",
     "iopub.status.busy": "2022-04-27T11:17:52.887412Z",
     "iopub.status.idle": "2022-04-27T11:17:54.493807Z",
     "shell.execute_reply": "2022-04-27T11:17:54.493125Z",
     "shell.execute_reply.started": "2022-04-27T11:17:52.887675Z"
    },
    "executionInfo": {
     "elapsed": 4156,
     "status": "ok",
     "timestamp": 1651025073215,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "ff1b043d",
    "outputId": "b113dbfe-ef1b-4f3c-b4f6-31723ab443d0"
   },
   "outputs": [],
   "source": [
    "X_train_genre, X_test_genre, y_train_genre, y_test_genre = train_test_split(X, y_genre, stratify=y_genre, test_size=0.10)\n",
    "\n",
    "# convert to tensor\n",
    "y_train_genre = torch.LongTensor(y_train_genre.values)\n",
    "y_test_genre = torch.LongTensor(y_test_genre.values)\n",
    "\n",
    "print(X_train_genre[0])\n",
    "print(y_train_genre[0])\n",
    "print(y_train_genre[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96c28255"
   },
   "source": [
    "#### Image vs Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:11:00.880832Z",
     "iopub.status.busy": "2022-04-27T12:11:00.880147Z",
     "iopub.status.idle": "2022-04-27T12:11:00.890719Z",
     "shell.execute_reply": "2022-04-27T12:11:00.889784Z",
     "shell.execute_reply.started": "2022-04-27T12:11:00.880793Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1651008893470,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "15bf125f",
    "outputId": "08c8c177-5239-4043-b92e-d08622d894dc"
   },
   "outputs": [],
   "source": [
    "y_style = df[\"style\"]\n",
    "print(y_style.unique())\n",
    "print(y_style.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "005e6494"
   },
   "source": [
    "The labels used for style are as follows:\n",
    "\n",
    "| Style | Label|\n",
    "| :-- | :-: |\n",
    "| Post-Impressionism | 0 |\n",
    "| Realism | 1 |\n",
    "| Impressionism | 2 |\n",
    "| Symbolism | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:11:03.059190Z",
     "iopub.status.busy": "2022-04-27T12:11:03.058569Z",
     "iopub.status.idle": "2022-04-27T12:11:03.085526Z",
     "shell.execute_reply": "2022-04-27T12:11:03.084045Z",
     "shell.execute_reply.started": "2022-04-27T12:11:03.059149Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1651008893470,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "3ad0efee",
    "outputId": "d0ba05e5-7c3b-4b82-d383-189e5b20e883"
   },
   "outputs": [],
   "source": [
    "labels_style = {}\n",
    "\n",
    "label = 0\n",
    "for style in y_style.unique():\n",
    "    labels_style[style] = label\n",
    "    label += 1 \n",
    "    \n",
    "for style, label in labels_style.items():\n",
    "    y_style = y_style.replace({style:label})\n",
    "\n",
    "print(y_style.unique())\n",
    "print(y_style.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:11:05.027203Z",
     "iopub.status.busy": "2022-04-27T12:11:05.026725Z",
     "iopub.status.idle": "2022-04-27T12:11:06.664172Z",
     "shell.execute_reply": "2022-04-27T12:11:06.663339Z",
     "shell.execute_reply.started": "2022-04-27T12:11:05.027152Z"
    },
    "executionInfo": {
     "elapsed": 1554,
     "status": "ok",
     "timestamp": 1651008895020,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "d1784195",
    "outputId": "9bbfddbe-8a60-4c82-cef5-4391ec9381f7"
   },
   "outputs": [],
   "source": [
    "X_train_style, X_test_style, y_train_style, y_test_style = train_test_split(X, y_style,stratify=y_style, test_size=0.10)\n",
    "\n",
    "# convert to tensor\n",
    "y_train_style = torch.LongTensor(y_train_style.values)\n",
    "y_test_style = torch.LongTensor(y_test_style.values)\n",
    "\n",
    "print(X_train_style[0])\n",
    "print(y_train_style[0])\n",
    "print(y_train_style[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bb0e0fd"
   },
   "source": [
    "#### Image vs Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:53:52.488911Z",
     "iopub.status.busy": "2022-04-27T11:53:52.488349Z",
     "iopub.status.idle": "2022-04-27T11:53:52.498863Z",
     "shell.execute_reply": "2022-04-27T11:53:52.498151Z",
     "shell.execute_reply.started": "2022-04-27T11:53:52.488871Z"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1651007234869,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "5ddeca88",
    "outputId": "536e6c10-891c-40d8-a5ae-5e029ed95e5a"
   },
   "outputs": [],
   "source": [
    "y_artist = df[\"artistName\"]\n",
    "print(y_artist.unique())\n",
    "print(y_artist.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "322119b4"
   },
   "source": [
    "The labels used for artists are as follows:\n",
    "\n",
    "| Style | Label|\n",
    "| :-- | :-: |\n",
    "| van Gogh Vincent | 0 |\n",
    "| Monet Claude | 1 |\n",
    "| Renoir Pierre-Auguste | 2 |\n",
    "| Roerich Nicholas | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:53:54.519109Z",
     "iopub.status.busy": "2022-04-27T11:53:54.518847Z",
     "iopub.status.idle": "2022-04-27T11:53:54.555406Z",
     "shell.execute_reply": "2022-04-27T11:53:54.554549Z",
     "shell.execute_reply.started": "2022-04-27T11:53:54.519079Z"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1651007234869,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "2f022920",
    "outputId": "a5032691-541e-4a02-fb82-b9f47f97269d"
   },
   "outputs": [],
   "source": [
    "labels_artist = {}\n",
    "\n",
    "label = 0\n",
    "for artist in y_artist.unique():\n",
    "    labels_artist[artist] = label\n",
    "    label += 1\n",
    "    \n",
    "for artist, label in labels_artist.items():\n",
    "    y_artist = y_artist.replace({artist:label})\n",
    "\n",
    "print(y_artist.unique())\n",
    "print(y_artist.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:53:57.000564Z",
     "iopub.status.busy": "2022-04-27T11:53:56.999783Z",
     "iopub.status.idle": "2022-04-27T11:53:58.562008Z",
     "shell.execute_reply": "2022-04-27T11:53:58.561336Z",
     "shell.execute_reply.started": "2022-04-27T11:53:57.000524Z"
    },
    "executionInfo": {
     "elapsed": 1865,
     "status": "ok",
     "timestamp": 1651007238905,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "8aa63005",
    "outputId": "58183a04-f670-49d3-ea98-ece7ed1266c4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_artist, X_test_artist, y_train_artist, y_test_artist = train_test_split(X, y_artist,stratify=y_artist, test_size=0.10)\n",
    "\n",
    "# convert to tensor\n",
    "y_train_artist = torch.LongTensor(y_train_artist.values)\n",
    "y_test_artist = torch.LongTensor(y_test_artist.values)\n",
    "\n",
    "print(X_train_artist[0])\n",
    "print(y_train_artist[0])\n",
    "print(y_train_artist[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be598380"
   },
   "source": [
    "### Make a three-layer net class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19731fd5"
   },
   "source": [
    "A deep network is likely required to for this predictive modeling problem, but we opt for a standard three-layer net class as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7e09484"
   },
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size):\n",
    "        super(three_layer_net , self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n",
    "        self.layer2 = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n",
    "        self.layer3 = nn.Linear(  hidden_size2 , output_size   , bias=False  )        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        z       = self.layer2(y_hat)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ea98a91"
   },
   "source": [
    "The input size of the net is determined by the size of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:11:15.813454Z",
     "iopub.status.busy": "2022-04-27T12:11:15.812975Z",
     "iopub.status.idle": "2022-04-27T12:11:15.818438Z",
     "shell.execute_reply": "2022-04-27T12:11:15.817601Z",
     "shell.execute_reply.started": "2022-04-27T12:11:15.813413Z"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1651025073216,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "475ee3d8",
    "outputId": "e8303b56-1080-41c2-8e6f-631928427150"
   },
   "outputs": [],
   "source": [
    "input_size = np.prod(size)\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f89fcbfa"
   },
   "source": [
    "We build the net with the following additional parameters for all attributes:\n",
    "- hidden size 1 = 900\n",
    "- hidden size 2 = 500\n",
    "- output size = 4 (four classes for each attribute)\n",
    "\n",
    "We do not wish to have an excess of nodes, lest the net has a tendency to overfit. These values were empirically determined to be a good trade-off between accuracy and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10109,
     "status": "ok",
     "timestamp": 1651000102658,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "36fd40e6",
    "outputId": "495fc448-4917-4561-c259-98fe7faf8145"
   },
   "outputs": [],
   "source": [
    "net_genre  = three_layer_net(input_size,900,500,4)\n",
    "net_genre = net_genre.to(device)\n",
    "print(net_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGFftAqSeP5g"
   },
   "outputs": [],
   "source": [
    "net_style  = three_layer_net(input_size,900,500,4)\n",
    "net_style = net_style.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2txLSUMkb8RU"
   },
   "outputs": [],
   "source": [
    "net_artist  = three_layer_net(input_size,900,500,4)\n",
    "net_artist = net_artist.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5575196"
   },
   "source": [
    "### Choose the criterion, optimizer, batch rate and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "018f4f81"
   },
   "source": [
    "We choose cross entropy loss as our loss criterion.\n",
    "Since our data is sparse, we choose an optimizer with a dynamic learning rate.\n",
    "\n",
    "Since we have sparse data, we wish to change the learning rate over time. While this can be done manually, we use [AdaDelta](https://paperswithcode.com/method/adadelta) as our optimizer to achieve this instead. AdaDelta performs sufficiently well in comparison with other commonly used optimizers like the stochastic gradient descent algorithm for the purposes of this project.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-28_at_4.18.37_PM.png)\n",
    "\n",
    "We do not have to set a default learning rate either. \n",
    "\n",
    "AdaDelta is, however, computationally more expensive as the second order derivatives have to be calculated, which might be a more significant issue with larger datasets. \n",
    "\n",
    "We set our batch size to 64. Note that the batch size is a factor of 2 so as to [maximize GPUs processing](https://www.intel.com/content/www/us/en/developer/articles/technical/cifar-10-classification-using-optimization-for-tensorflow.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ede51da8"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_genre = torch.optim.Adadelta(net_genre.parameters())\n",
    "optimizer_style = torch.optim.Adadelta(net_style.parameters())\n",
    "optimizer_artist = torch.optim.Adadelta(net_artist.parameters())\n",
    "\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9c2e965"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "431f8b8d"
   },
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fa87242",
    "outputId": "f586cc47-cd2d-4f60-b2c8-02c633085ea7"
   },
   "outputs": [],
   "source": [
    "for iter in range(1,5000):\n",
    "    \n",
    "    # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "    optimizer_genre.zero_grad()\n",
    "     \n",
    "    # create a minibatch\n",
    "    indices = torch.LongTensor(bs).random_(0, 11440)\n",
    "    minibatch_data =  X_train_genre[indices]\n",
    "    minibatch_label = y_train_genre[indices]\n",
    "\n",
    "    minibatch_data = minibatch_data.to(device)\n",
    "    minibatch_label = minibatch_label.to(device)\n",
    "    \n",
    "    #reshape the minibatch\n",
    "    inputs = minibatch_data.view(bs, 3*120*150)\n",
    "    \n",
    "    # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "    inputs.requires_grad_()\n",
    "\n",
    "    # forward the minibatch through the net  \n",
    "    scores = net_genre(inputs) \n",
    "    \n",
    "    # Compute the average of the losses of the data points in the minibatch\n",
    "    loss = criterion(scores, minibatch_label) \n",
    "    \n",
    "    # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "    loss.backward()\n",
    "    \n",
    "    # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "    optimizer_genre.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0a0f15a"
   },
   "outputs": [],
   "source": [
    "torch.save(net_genre.state_dict(), os.path.join(current_folder,'net_genre_mlp.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13c9e684"
   },
   "source": [
    "#### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 170546,
     "status": "ok",
     "timestamp": 1650986958889,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "332ea368",
    "outputId": "74f7d24b-4829-4294-a7d0-4805d4f364b7"
   },
   "outputs": [],
   "source": [
    "for iter in range(1,5000):\n",
    "    \n",
    "    # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "    optimizer_style.zero_grad()\n",
    "     \n",
    "    # create a minibatch\n",
    "    indices = torch.LongTensor(bs).random_(0, 11440)\n",
    "    minibatch_data =  X_train_style[indices]\n",
    "    minibatch_label = y_train_style[indices]\n",
    "\n",
    "    minibatch_data = minibatch_data.to(device)\n",
    "    minibatch_label = minibatch_label.to(device)\n",
    "    \n",
    "    #reshape the minibatch\n",
    "    inputs = minibatch_data.view(bs, 3*120*150)\n",
    "    \n",
    "    # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "    inputs.requires_grad_()\n",
    "\n",
    "    # forward the minibatch through the net  \n",
    "    scores = net_style(inputs) \n",
    "    \n",
    "    # Compute the average of the losses of the data points in the minibatch\n",
    "    loss = criterion(scores, minibatch_label) \n",
    "    \n",
    "    # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "    loss.backward()\n",
    "    \n",
    "    # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "    optimizer_style.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qhskLK-foCD"
   },
   "outputs": [],
   "source": [
    "torch.save(net_style.state_dict(), os.path.join(current_folder,'net_style_mlp.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHkse_nQco_C"
   },
   "source": [
    "#### Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 170803,
     "status": "ok",
     "timestamp": 1650986511737,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "Wynkqlb6cvWi",
    "outputId": "73523d7b-746d-415e-992a-95cc3c43eb40"
   },
   "outputs": [],
   "source": [
    "for iter in range(1,5000):\n",
    "    \n",
    "    # Set dL/dU, dL/dV, dL/dW to be filled with zeros\n",
    "    optimizer_artist.zero_grad()\n",
    "     \n",
    "    # create a minibatch\n",
    "    indices = torch.LongTensor(bs).random_(0, 11440)\n",
    "    minibatch_data =  X_train_artist[indices]\n",
    "    minibatch_label = y_train_artist[indices]\n",
    "\n",
    "    minibatch_data = minibatch_data.to(device)\n",
    "    minibatch_label = minibatch_label.to(device)\n",
    "    \n",
    "    #reshape the minibatch\n",
    "    inputs = minibatch_data.view(bs, 3*120*150)\n",
    "    \n",
    "    # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "    inputs.requires_grad_()\n",
    "\n",
    "    # forward the minibatch through the net  \n",
    "    scores = net_artist(inputs) \n",
    "    \n",
    "    # Compute the average of the losses of the data points in the minibatch\n",
    "    loss = criterion(scores, minibatch_label) \n",
    "    \n",
    "    # backward pass to compute dL/dU, dL/dV and dL/dW    \n",
    "    loss.backward()\n",
    "    \n",
    "    # do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...\n",
    "    optimizer_artist.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T73kmSxodhmU"
   },
   "outputs": [],
   "source": [
    "torch.save(net_artist.state_dict(), os.path.join(current_folder,'net_artist_mlp.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22a01ee9"
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cf8890a"
   },
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc1ddc3a"
   },
   "source": [
    "Testing on a random image yields the correct class label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac11441c",
    "outputId": "beeec5b3-8fdd-4d83-e433-6913ae46630c"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_genre_size = len(X_test_genre)\n",
    "\n",
    "idx = randint(0, test_genre_size)\n",
    "im = X_test_genre[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_genre(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_genre[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6210a384"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 70.99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3a731ea",
    "outputId": "637fc738-6621-4238-bc52-43d44a53046f"
   },
   "outputs": [],
   "source": [
    "accuracy = [0, 0]\n",
    "\n",
    "for X_temp, y_temp in zip(X_test_genre, y_test_genre):\n",
    "    scores =  net_genre(X_temp.view(1,54000).to(device)) \n",
    "    probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "    if torch.argmax(probs) == y_temp:\n",
    "        accuracy[0] += 1\n",
    "    else:\n",
    "        accuracy[1] += 1\n",
    "\n",
    "print(\"Accuracy is {:.2f}%\".format(accuracy[0] / (accuracy[0] + accuracy[1]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQoHuXmOgNo7"
   },
   "source": [
    "#### Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NassmRx_gPqo"
   },
   "source": [
    "Testing on a random image yields the correct class label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1650987220186,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "py3abjbWgRq3",
    "outputId": "a450ea91-5692-48b8-a6bd-82870cc6af7a"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_style_size = len(X_test_style)\n",
    "\n",
    "idx = randint(0, test_style_size)\n",
    "im = X_test_style[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_style(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_style[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqKzURVtgjFM"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 64.39%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1987,
     "status": "ok",
     "timestamp": 1650987282694,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "MBAoWOxYgj9Q",
    "outputId": "1d2b3ce6-7d63-401d-8f30-52771f9a5e4f"
   },
   "outputs": [],
   "source": [
    "accuracy = [0, 0]\n",
    "\n",
    "for X_temp, y_temp in zip(X_test_style, y_test_style):\n",
    "    scores =  net_style(X_temp.view(1,54000).to(device)) \n",
    "    probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "    if torch.argmax(probs) == y_temp:\n",
    "        accuracy[0] += 1\n",
    "    else:\n",
    "        accuracy[1] += 1\n",
    "\n",
    "print(\"Accuracy is {:.2f}%\".format(accuracy[0] / (accuracy[0] + accuracy[1]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTQ06bpLhIdb"
   },
   "source": [
    "#### Artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--eBCLS3hJ_J"
   },
   "source": [
    "Testing on a random image yields the correct class label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1525,
     "status": "ok",
     "timestamp": 1650987884145,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "Vrvbzof-hPCh",
    "outputId": "af6b40d7-3212-4138-c0e1-17b64f4d63dc"
   },
   "outputs": [],
   "source": [
    "net_artist = three_layer_net(input_size,900,500,4).to(device)#torch.load(os.path.join(current_folder,'net_artist_mlp.pth'))\n",
    "#model = Model()\n",
    "net_artist.load_state_dict(torch.load(os.path.join(current_folder,'net_artist_mlp.pth')))\n",
    "net_artist.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1650987886331,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "-eeML8qxhcLR",
    "outputId": "05d106fc-b773-413f-a9da-1d98f49c6b9f"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_artist_size = len(X_test_artist)\n",
    "\n",
    "idx = randint(0, test_artist_size)\n",
    "im = X_test_artist[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_artist(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_artist[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLNvV9SAjDOm"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 67.37%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1566,
     "status": "ok",
     "timestamp": 1650987942943,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "10Pn0RIyjE9g",
    "outputId": "369bd876-13c0-450a-8f4f-c50e8107454e"
   },
   "outputs": [],
   "source": [
    "accuracy = [0, 0]\n",
    "\n",
    "for X_temp, y_temp in zip(X_test_artist, y_test_artist):\n",
    "    scores =  net_artist(X_temp.view(1,54000).to(device)) \n",
    "    probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "    if torch.argmax(probs) == y_temp:\n",
    "        accuracy[0] += 1\n",
    "    else:\n",
    "        accuracy[1] += 1\n",
    "\n",
    "print(\"Accuracy is {:.2f}%\".format(accuracy[0] / (accuracy[0] + accuracy[1]) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVsqogTSmoPA"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbWXwASimpio"
   },
   "source": [
    "In all three instances, the model can be further tweaked (e.g. one more hidden layer, varying the number of neurons in each layer) to achieve better results, but we shall treat this as a baseline for the next few models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DTPPP0Wm-S6"
   },
   "source": [
    "## Vanilla CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfEjtbSHoq2S"
   },
   "source": [
    "We first define a general-purpose error function, as well as a function that evaluates the accuracy of the model on the test set that we will use to evaluate the progress of the models-in-training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:11:36.499772Z",
     "iopub.status.busy": "2022-04-27T12:11:36.499492Z",
     "iopub.status.idle": "2022-04-27T12:11:36.505805Z",
     "shell.execute_reply": "2022-04-27T12:11:36.504589Z",
     "shell.execute_reply.started": "2022-04-27T12:11:36.499741Z"
    },
    "id": "dTCCFNbBtdGR"
   },
   "outputs": [],
   "source": [
    "# credit to https://github.com/xbresson/CS4243_2022/blob/main/codes/labs_lecture06/lab02_cifar_multilayer/cifar_multilayer_solution.ipynb by xbresson\n",
    "def get_error(scores, labels):\n",
    "\n",
    "    bs = scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches = indicator.sum()\n",
    "    \n",
    "    return 1 - num_matches.float()/bs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:14:37.289211Z",
     "iopub.status.busy": "2022-04-27T12:14:37.288532Z",
     "iopub.status.idle": "2022-04-27T12:14:37.297903Z",
     "shell.execute_reply": "2022-04-27T12:14:37.297077Z",
     "shell.execute_reply.started": "2022-04-27T12:14:37.289169Z"
    },
    "id": "47vYfUGNoz6O"
   },
   "outputs": [],
   "source": [
    "# credit to https://github.com/xbresson/CS4243_2022/blob/main/codes/labs_lecture06/lab02_cifar_multilayer/cifar_multilayer_solution.ipynb by xbresson\n",
    "def eval_on_test_set(net, attribute, bs):\n",
    "\n",
    "    # choose appropriate test data\n",
    "    if attribute == 'genre':\n",
    "        test_data = X_test_genre\n",
    "        test_label = y_test_genre\n",
    "    elif attribute == 'style':\n",
    "        test_data = X_test_style\n",
    "        test_label = y_test_style\n",
    "    else:\n",
    "        test_data = X_test_artist\n",
    "        test_label = y_test_artist\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    test_label = test_label.to(device)\n",
    "\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    test_set_size = len(test_label)\n",
    "\n",
    "    for i in range(0, test_set_size, bs):\n",
    "\n",
    "        minibatch_data  =  test_data[i:i+bs]\n",
    "        minibatch_label =  test_label[i:i+bs]\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        scores = net(inputs) \n",
    "\n",
    "        error = get_error(scores, minibatch_label)\n",
    "\n",
    "        running_error += error.item()\n",
    "\n",
    "        num_batches+=1\n",
    "\n",
    "\n",
    "    total_error = running_error/num_batches\n",
    "    print(\"Error rate on test set: {:.2f}%\".format(total_error*100))\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFimvulXi_Jh"
   },
   "source": [
    "We use the same training/testing data splits, as well as the same model structure for each attribute as in the earlier section on MLPs. \n",
    "\n",
    "We opt for the Adam optimizer in this instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SXJIZzcuV15"
   },
   "source": [
    "We do 50 passes through the training set for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:18:59.667605Z",
     "iopub.status.busy": "2022-04-27T11:18:59.667068Z",
     "iopub.status.idle": "2022-04-27T11:18:59.671645Z",
     "shell.execute_reply": "2022-04-27T11:18:59.670902Z",
     "shell.execute_reply.started": "2022-04-27T11:18:59.667570Z"
    },
    "id": "l25gOvqfnC2Y"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJdYr87FjSiG"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bF2d5tf35E6"
   },
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyBRjck7jkd4"
   },
   "source": [
    "It was empirically observed that a learning rate of 0.01 was prone to non-convergence; the error rate on the test set fluctuated at around ~50% and the loss per epoch remained fairly constant for 200 epoches, with all other parameters being the same.\n",
    "\n",
    "A learning rate of 0.001 was thus chosen for predicting genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJLYdT8QVeD-"
   },
   "outputs": [],
   "source": [
    "net_genre = three_layer_net(input_size,900,500,4)\n",
    "net_genre = net_genre.to(device)\n",
    "optimizer_genre = torch.optim.Adam(net_genre.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 277010,
     "status": "ok",
     "timestamp": 1651005994530,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "7gbCDpgDrzHv",
    "outputId": "9a74d58c-d6e8-411a-e8a5-21f134265b35"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_genre = []\n",
    "error_per_epoch_genre = []\n",
    "test_error_per_epoch_genre = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_genre = len(y_train_genre)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_genre)\n",
    " \n",
    "    for count in range(0, train_size_genre, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_genre.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_genre[indices]\n",
    "        minibatch_label =  y_train_genre[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_genre(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_genre.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_genre.append(eval_on_test_set(net_genre, 'genre', bs))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzj2Gd9forrB"
   },
   "outputs": [],
   "source": [
    "torch.save(net_genre.state_dict(), os.path.join(current_folder,'net_genre_vanilla_cnn.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_I1o-0bo_c8"
   },
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(loss_per_epoch_genre), os.path.join(current_folder,'loss_per_epoch_genre_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(error_per_epoch_genre), os.path.join(current_folder,'error_per_epoch_genre_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(test_error_per_epoch_genre), os.path.join(current_folder,'test_error_per_epoch_genre_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slXuOxmekIlV"
   },
   "source": [
    "#### Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrnPr6aaoeBT"
   },
   "source": [
    "We repeat the process for the style and artist attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "error",
     "timestamp": 1651008921037,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "9pb4ZGMYohBh",
    "outputId": "0f7de344-f467-462c-b336-d6f8cad99963"
   },
   "outputs": [],
   "source": [
    "net_style = three_layer_net(input_size,900,500,4)\n",
    "net_style = net_style.to(device)\n",
    "optimizer_style = torch.optim.Adam(net_style.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283218,
     "status": "ok",
     "timestamp": 1651007038531,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "lqmaYVlxqmyX",
    "outputId": "457e47d4-6de4-457b-fca4-76763837db26"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_style = []\n",
    "error_per_epoch_style = []\n",
    "test_error_per_epoch_style = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_style = len(y_train_style)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_style)\n",
    " \n",
    "    for count in range(0, train_size_style, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_style.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_style[indices]\n",
    "        minibatch_label =  y_train_style[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_style(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_style.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_style.append(eval_on_test_set(net_style, 'style', bs))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OTJpyMLrB4p"
   },
   "outputs": [],
   "source": [
    "torch.save(net_style.state_dict(), os.path.join(current_folder,'net_style_vanilla_cnn.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fQi6kITrGtb"
   },
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(loss_per_epoch_style), os.path.join(current_folder,'loss_per_epoch_style_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(error_per_epoch_style), os.path.join(current_folder,'error_per_epoch_style_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(test_error_per_epoch_style), os.path.join(current_folder,'test_error_per_epoch_style_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjPPvAaKsKmZ"
   },
   "source": [
    "#### Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1hH1kR5sMWa"
   },
   "outputs": [],
   "source": [
    "net_artist = three_layer_net(input_size,900,500,4)\n",
    "net_artist = net_artist.to(device)\n",
    "optimizer_artist = torch.optim.Adam(net_artist.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 279098,
     "status": "ok",
     "timestamp": 1651007565119,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "nD-ZYUMxsXyK",
    "outputId": "99e65ee6-d1d0-4449-8d6b-44fc2e8ae3d4"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_artist = []\n",
    "error_per_epoch_artist = []\n",
    "test_error_per_epoch_artist = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_artist = len(y_train_artist)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_artist)\n",
    " \n",
    "    for count in range(0, train_size_artist, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_artist.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_artist[indices]\n",
    "        minibatch_label =  y_train_artist[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_artist(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_artist.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_artist.append(eval_on_test_set(net_artist, 'artist', bs))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRbbrN_juLHq"
   },
   "outputs": [],
   "source": [
    "torch.save(net_artist.state_dict(), os.path.join(current_folder,'net_artist_vanilla_cnn.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4Wz75kKuPr6"
   },
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(loss_per_epoch_artist), os.path.join(current_folder,'loss_per_epoch_artist_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(error_per_epoch_artist), os.path.join(current_folder,'error_per_epoch_artist_vanilla_cnn.pt'))\n",
    "torch.save(torch.Tensor(test_error_per_epoch_artist), os.path.join(current_folder,'test_error_per_epoch_artist_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPJGlzu5uqdx"
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvI9Zb1fvGlx"
   },
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5813,
     "status": "ok",
     "timestamp": 1651008230151,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "mUJfxzDKwen7",
    "outputId": "3440cae0-61bc-41c4-a006-f4e2c5e06128"
   },
   "outputs": [],
   "source": [
    "net_genre = three_layer_net(input_size,900,500,4).to(device)\n",
    "#model = Model()\n",
    "net_genre.load_state_dict(torch.load(os.path.join(current_folder,'net_genre_mlp.pth')))\n",
    "net_genre.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxl5Q8rCu63Y"
   },
   "source": [
    "Testing on a random image yields the correct class label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1651008301620,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "aAkR1A_au4WK",
    "outputId": "844a36fb-14b1-4ca7-c0b0-babc149da25d"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_genre_size = len(X_test_genre)\n",
    "\n",
    "idx = randint(0, test_genre_size)\n",
    "im = X_test_genre[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_genre(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_genre[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u46_QtZ6xArD"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 71.04%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1651008326087,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "nrrMFVLju0Ea",
    "outputId": "140d6207-bd61-4a7b-846e-99329013f98c"
   },
   "outputs": [],
   "source": [
    "test_error = eval_on_test_set(net_genre, 'genre', bs)\n",
    "print(\"Accuracy is {:.2f}%\".format(100-test_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_5AS9BgvNO1"
   },
   "source": [
    "#### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12109,
     "status": "ok",
     "timestamp": 1651008948833,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "StbSOJwUxjWh",
    "outputId": "d5d99a0b-3986-4803-cf9a-00f85a2d077e"
   },
   "outputs": [],
   "source": [
    "net_style = three_layer_net(input_size,900,500,4).to(device)\n",
    "#model = Model()\n",
    "net_style.load_state_dict(torch.load(os.path.join(current_folder,'net_style_mlp.pth')))\n",
    "net_style.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9N9yu8BvRAu"
   },
   "source": [
    "Testing on a random image yields an incorrect class label prediction - class 2 is predicted instead of 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1304,
     "status": "ok",
     "timestamp": 1651008982041,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "xpqPOanVvRAu",
    "outputId": "7ce2b718-1623-4c9f-997b-5bedba491f70"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_style_size = len(X_test_style)\n",
    "\n",
    "idx = randint(0, test_style_size)\n",
    "im = X_test_style[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_style(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_style[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDUUfIPNzsjF"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 66.90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1651009036814,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "YCa1vIZHvRAv",
    "outputId": "32ca871c-9034-4d40-afe6-77f719c13972"
   },
   "outputs": [],
   "source": [
    "test_error = eval_on_test_set(net_style, 'style', bs)\n",
    "print(\"Accuracy is {:.2f}%\".format(100-test_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSfIpv6WvRAu"
   },
   "source": [
    "#### Artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whtd5uWtvNO2"
   },
   "source": [
    "Testing on a random image yields the correct class label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1651007942670,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "QMQGJJAQvNO2",
    "outputId": "21bfd0bc-15d3-46c1-d708-2044f1154adf"
   },
   "outputs": [],
   "source": [
    "# choose a picture at random\n",
    "test_artist_size = len(X_test_artist)\n",
    "\n",
    "idx = randint(0, test_artist_size)\n",
    "im = X_test_artist[idx]\n",
    "\n",
    "# diplay the picture\n",
    "#utils.show(im)\n",
    "\n",
    "# feed it to the net and display the confidence scores\n",
    "scores =  net_artist(im.view(1,54000).to(device)) \n",
    "probs = torch.softmax(scores, dim=1)\n",
    "\n",
    "print(scores)\n",
    "print(probs)\n",
    "print(y_test_artist[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAiCEirxwNOz"
   },
   "source": [
    "Testing the model on the test set yields at accuracy of 56.81%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1651008076000,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "LhMI_SzuvNO3",
    "outputId": "89cdd88d-3b7f-4090-a646-41c4f1bbb4f0"
   },
   "outputs": [],
   "source": [
    "test_error = eval_on_test_set(net_artist, 'artist', bs)\n",
    "print(\"Accuracy is {:.2f}%\".format(100-test_error*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8puC8cHfz1n5"
   },
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbPJ9m09z9Ch"
   },
   "source": [
    "The tabulated accuracy scores of the models on the test sets are as follows.\n",
    "\n",
    "| Model\\ Label | Genre | Style | Artist |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| MLP | 70.99% | 64.39% | 67.37% |\n",
    "| CNN | 71.04% | 66.90% | 56.81 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3cnmueL1bNG"
   },
   "source": [
    "Genre appears to be consistently easier to identify across the models, while style and artist appear to be harder to classify. This seems to follow intuition, for the chosen genre here - sketch and study, portrait, genre painting, landscape - are more easy to distinguish to the typical human eye as opposed to, for instance, post-impressionism and impressionism, as is in the case of style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyJsMsUD5buL"
   },
   "source": [
    "Do note that while the testing data is used as validation data in this project, ideally, there would be a separate set of testing data for more accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdTrvWzS8iW5"
   },
   "source": [
    "The CNN models, while having about the same accuracy as the MLP models in this particular implementation, appeared to be more volatile when subjected to changes in parameters; changing the learning rate from 0.01 to 0.001 alone increased the accuracy of the CNN for genre from ~50% to over 70%. \n",
    "\n",
    "As such, it is likely that with more training data and fine-tuned parameters, they will outperform the MLP models. \n",
    "\n",
    "This is an expected outcome given the nature of both models. MLP utilizes vector input whereas CNN utilizes tensor input, resulting in CNN models having a better capability to understand spatial relations across rows and columns. Since spatial relations between pixels matter more for the identification of features in image classification, CNN is expected to perform better than MLP with the right parameters and more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYX_Ge7I65xr"
   },
   "source": [
    "Plotting the change of loss and running error of epoches in the CNN models over time reveals overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0] + \"\\\\\"\n",
    "try:\n",
    "  current_folder = path_to_file\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_Project/'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "N64RLNHq6dzr"
   },
   "outputs": [],
   "source": [
    "error_per_epoch_genre = torch.load(os.path.join(current_folder,'error_per_epoch_genre_vanilla_cnn.pt'))\n",
    "test_error_per_epoch_genre = torch.load(os.path.join(current_folder,'test_error_per_epoch_genre_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1651018934011,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "2Wy-xcQR7GkT",
    "outputId": "c72a9c23-ccc7-40a5-ba83-b0ea37144153"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Error per epoch",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          78.21353912353516,
          68.88021087646484,
          67.09896087646484,
          66.46875,
          65.47396087646484,
          64.89583587646484,
          63.546875,
          61.640625,
          61.46875,
          59.609375,
          60.52083206176758,
          59.39583206176758,
          57.33333206176758,
          57.6875,
          58.53645706176758,
          56.82291793823242,
          54.828125,
          55.828125,
          55.171875,
          53.92708206176758,
          53.64583206176758,
          53.94791793823242,
          52.79166793823242,
          53.83854293823242,
          53.375,
          52.97916793823242,
          52.234375,
          51.5625,
          51.61458206176758,
          50.14583206176758,
          49.59895706176758,
          51.796875,
          49.89583206176758,
          49.765625,
          49.97395706176758,
          50.56770706176758,
          48.16145706176758,
          48.19791793823242,
          48.546875,
          46.78125,
          46.16666793823242,
          49.05208206176758,
          48.66666793823242,
          49.69791793823242,
          49.453125,
          46.34375,
          46.24479293823242,
          45.17708206176758,
          44.875,
          44.953125
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Test error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Test error per epoch",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Test error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          39.654014587402344,
          40,
          37.35491180419922,
          36.28348159790039,
          36.27231979370117,
          34.140625,
          33.50446701049805,
          38.75,
          34.54241180419922,
          34.59821319580078,
          33.99553680419922,
          34.16294860839844,
          31.953126907348633,
          34.52008819580078,
          32.41071319580078,
          34.87723159790039,
          31.350444793701172,
          32.36606979370117,
          31.886159896850586,
          30.412944793701172,
          34.72098159790039,
          33.203125,
          30.803569793701172,
          34.48660659790039,
          32.77901840209961,
          32.12053298950195,
          32.93526840209961,
          33.06919479370117,
          30.915176391601562,
          32.622764587402344,
          31.75223159790039,
          31.171876907348633,
          33.62723159790039,
          33.30356979370117,
          31.674108505249023,
          31.886159896850586,
          34.174110412597656,
          32.55580520629883,
          32.77901840209961,
          35.12276840209961,
          37.80134201049805,
          33.29240798950195,
          32.83482360839844,
          35.47990798950195,
          35.56919860839844,
          32.37723159790039,
          31.875,
          33.14731979370117,
          32.97991180419922,
          32.92410659790039
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Genre"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "# normalize training and test data error\n",
    "test_error_per_epoch_genre = torch.mul(test_error_per_epoch_genre, 100)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(np.column_stack([error_per_epoch_genre, test_error_per_epoch_genre]), \\\n",
    "                  columns=['Error per epoch', 'Test error per epoch'])\n",
    "\n",
    "# add index\n",
    "df['index'] = df.index + 1 \n",
    "\n",
    "fig = px.line(df, x='index', y=[\"Error per epoch\",\"Test error per epoch\"], \\\n",
    "              labels={'index':'Epoch', 'Error per epoch':'Error per epoch', \"Test error per epoch\":\"Test error per epoch\"}, title=\"Genre\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzhG1ZkkF3uL"
   },
   "source": [
    "Note that the error rate for the test set is slowly starting to increase whereas that of the training set is still steadily decreasing by the 50th epoch. This pattern is observed well after the 50th epoch into the 200th epoch as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiNma-jCG8DL"
   },
   "source": [
    "Plotting similar graphs for style and artist show a similar - perhaps milder - pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EwdL9KDYHLXK"
   },
   "outputs": [],
   "source": [
    "error_per_epoch_style = torch.load(os.path.join(current_folder,'error_per_epoch_style_vanilla_cnn.pt'))\n",
    "test_error_per_epoch_style = torch.load(os.path.join(current_folder,'test_error_per_epoch_style_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1651018943055,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "tK650_PTHPn0",
    "outputId": "564c07df-8e16-4da8-d193-417ce443ba90"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Error per epoch",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          87.328125,
          77.34375,
          74.65103912353516,
          73.1875,
          73.27083587646484,
          70.39583587646484,
          69.97396087646484,
          67.19791412353516,
          68.19791412353516,
          66.43228912353516,
          65.38541412353516,
          64.30728912353516,
          64.49478912353516,
          64.38541412353516,
          63.125,
          63.99479293823242,
          61.015625,
          62.65625,
          61.47395706176758,
          60.29166793823242,
          59.19791793823242,
          59.41145706176758,
          60.50520706176758,
          59.44270706176758,
          58.21875,
          58.6875,
          58.19791793823242,
          56.5,
          56.11458206176758,
          57.796875,
          56.5625,
          56.44270706176758,
          55.703125,
          52.04166793823242,
          52.77604293823242,
          53.94270706176758,
          52.54166793823242,
          52.109375,
          50.47916793823242,
          52.58854293823242,
          52.21875,
          51.29166793823242,
          49.47916793823242,
          49.63541793823242,
          47.265625,
          48.43229293823242,
          49.59895706176758,
          48.08854293823242,
          49.09895706176758,
          47.6875
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Test error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Test error per epoch",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Test error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          45.71428680419922,
          45.63615798950195,
          42.41071319580078,
          38.89508819580078,
          39.87723159790039,
          37.83481979370117,
          40.424110412597656,
          39.19643020629883,
          36.06026840209961,
          37.32142639160156,
          37.46651840209961,
          36.45089340209961,
          37.44419479370117,
          35.51339340209961,
          37.24330520629883,
          34.27455520629883,
          36.18303680419922,
          35.56919860839844,
          35.747764587402344,
          36.529014587402344,
          34.63169479370117,
          39.375,
          37.95758819580078,
          34.375,
          35.736610412597656,
          37.65625,
          37.46651840209961,
          37.16517639160156,
          37.06473159790039,
          35.859375,
          38.984375,
          36.61830520629883,
          38.76115798950195,
          38.21428298950195,
          37.08705139160156,
          37.87946319580078,
          38.31473159790039,
          34.43080139160156,
          36.77455139160156,
          38.53794479370117,
          38.41518020629883,
          34.28571701049805,
          33.93973159790039,
          35.40178680419922,
          34.966514587402344,
          34.88839340209961,
          38.861610412597656,
          35.46875,
          37.265625,
          36.76339340209961
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Style"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "# normalize training and test data error\n",
    "test_error_per_epoch_style = torch.mul(test_error_per_epoch_style, 100)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(np.column_stack([error_per_epoch_style, test_error_per_epoch_style]), \\\n",
    "                  columns=['Error per epoch', 'Test error per epoch'])\n",
    "\n",
    "# add index\n",
    "df['index'] = df.index + 1 \n",
    "\n",
    "fig = px.line(df, x='index', y=[\"Error per epoch\",\"Test error per epoch\"], \\\n",
    "              labels={'index':'Epoch', 'Error per epoch':'Error per epoch', \"Test error per epoch\":\"Test error per epoch\"}, title=\"Style\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LbRiZQWyHvFg"
   },
   "outputs": [],
   "source": [
    "error_per_epoch_artist = torch.load(os.path.join(current_folder,'error_per_epoch_artist_vanilla_cnn.pt'))\n",
    "test_error_per_epoch_artist = torch.load(os.path.join(current_folder,'test_error_per_epoch_artist_vanilla_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1027,
     "status": "ok",
     "timestamp": 1651018953565,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "p4G2tgMsH0qy",
    "outputId": "d8014a2b-47d3-4451-bd0d-70fa5eccb59e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=Error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Error per epoch",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          103.09375,
          87.33853912353516,
          87.40103912353516,
          83.74478912353516,
          81.4375,
          78.66146087646484,
          81.203125,
          77.34375,
          77.27603912353516,
          74.890625,
          74.60416412353516,
          73.78646087646484,
          73.703125,
          73.78125,
          70.578125,
          71.265625,
          70.52083587646484,
          70.72396087646484,
          68.61978912353516,
          66.49478912353516,
          66.515625,
          67.16146087646484,
          65.890625,
          65.625,
          64.78125,
          65.76041412353516,
          65.08853912353516,
          63.40104293823242,
          64.16666412353516,
          61.18229293823242,
          61.59895706176758,
          62.05208206176758,
          60.67708206176758,
          60.82291793823242,
          58.32291793823242,
          59.11979293823242,
          57.33854293823242,
          56.93229293823242,
          55.35416793823242,
          57.55729293823242,
          56.04166793823242,
          55.5625,
          54.765625,
          55.85416793823242,
          54.44791793823242,
          51.79166793823242,
          51.93229293823242,
          50.77083206176758,
          50.90625,
          49.765625
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "variable=Test error per epoch<br>Epoch=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "Test error per epoch",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Test error per epoch",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50
         ],
         "xaxis": "x",
         "y": [
          50.81473159790039,
          48.359375,
          48.73884201049805,
          49.017860412597656,
          46.46205139160156,
          50.77009201049805,
          46.41740798950195,
          44.609375,
          47.43303680419922,
          45.55803680419922,
          43.90625,
          45.15625,
          43.83928298950195,
          45.78125,
          45.34598159790039,
          42.83481979370117,
          45.27901840209961,
          44.88839340209961,
          42.86830520629883,
          43.52678680419922,
          44.58705139160156,
          43.560264587402344,
          43.89508819580078,
          43.31473159790039,
          43.71651840209961,
          43.24776840209961,
          43.05803298950195,
          43.34821319580078,
          42.23214340209961,
          41.07142639160156,
          44.85490798950195,
          44.15178680419922,
          43.03571319580078,
          41.640625,
          42.43303680419922,
          42.34375,
          44.49776840209961,
          41.595985412597656,
          41.015625,
          42.47768020629883,
          43.23660659790039,
          44.17410659790039,
          43.38169860839844,
          40.37946319580078,
          41.74106979370117,
          42.29910659790039,
          43.61606979370117,
          42.36607360839844,
          40.47991180419922,
          43.19196319580078
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Artist"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "# normalize training and test data error\n",
    "test_error_per_epoch_artist = torch.mul(test_error_per_epoch_artist, 100)\n",
    "\n",
    "# create dataframe\n",
    "df = pd.DataFrame(np.column_stack([error_per_epoch_artist, test_error_per_epoch_artist]), \\\n",
    "                  columns=['Error per epoch', 'Test error per epoch'])\n",
    "\n",
    "# add index\n",
    "df['index'] = df.index + 1 \n",
    "\n",
    "fig = px.line(df, x='index', y=[\"Error per epoch\",\"Test error per epoch\"], \\\n",
    "              labels={'index':'Epoch', 'Error per epoch':'Error per epoch', \"Test error per epoch\":\"Test error per epoch\"}, title=\"Artist\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3TncAw1IMAy"
   },
   "source": [
    "The huge discrepancy between the accuracy of the test set and the training set is an indicator of possible overfitting as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z07OmY2NIe2a"
   },
   "source": [
    "### Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUNAtASrIk26"
   },
   "source": [
    "#### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWUcJ23sIz4C"
   },
   "source": [
    "Overfitting occurs as a result of the model being too closely aligned with the training set. An overfitted model fails to generalize well from the training data used to new data.\n",
    "\n",
    "In the above models, we use early-stopping as a means of preventing the model from being too overfitted to the training data, but the models still show a large discrepency between their prediction error on the training set and on the testing set. \n",
    "\n",
    "To counter this, we can force the models to be artificially simpler by regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zERxa7ysKQ0z"
   },
   "source": [
    "1. Dropout\n",
    "  \n",
    "  ![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png)\n",
    "\n",
    "  Dropout is a regularisation technique that randomly shuts off neurons in a layer. The probability of any given neuron being shut off in a dropout layer, *p*, can be specified by the user. \n",
    "\n",
    "  Dropout forces the model to adapt to the loss of certain neurons, such that the complexity of the resulting model is reduced, as shown in the image above. This prevents overfitting with an appropriately chosen *p*, but can oversimplify the model if *p* is too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxSSlfhHMCll"
   },
   "source": [
    "2. Batch normalization\n",
    "\n",
    "  ![](https://images.deepai.org/glossary-terms/981e1ffea3814ae193c27461253faf63/batch_normalization.png)\n",
    "\n",
    "  Deep learning methods, including CNNs, are sensitive to the values of initial random weights.\n",
    "  \n",
    "  The presence of drastically large weights (by relativity to other weights) in a model can have a cascading effect through the neural network during training, causing the network to be more sensitive to the features with larger values and the feature activation distribution to shift towards larger values. Note that ReLU does not help in such cases as well. While this is by design of a neural network, an drastically large weight might result in the loss of information from other weights in the same layer over time.\n",
    "\n",
    "  Batch normalization refers to the process of normalizing data for each layer in a network.\n",
    "\n",
    "  Batch normalization is typically applied before an activation layer if the activation layer is ReLU. This is a three step process:\n",
    "  \n",
    "  1. Normalize output of layer\n",
    "  2. Multiply normalized output by a “standard deviation” parameter $γ$\n",
    "  3. Add a “mean” parameter $β$ to the output of (2)\n",
    "  \n",
    "  $x \\mapsto \\big( \\frac{x-m}{s}\\big)γ + β$\n",
    "\n",
    "  $γ$ and β are also optimized during the training process. This ensures that the weights within the network are more balanced. \n",
    "\n",
    "  Batch normalization can increase the speed of training as well. \n",
    "\n",
    "  Dropout with batch normalization often lead to worse results in performance when applied together due to [variance shift differences](https://arxiv.org/pdf/1801.05134.pdf). As such, these techniques will have to be applied to separate models if used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pgkBB3OUVIM"
   },
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNAiph_eUYEv"
   },
   "source": [
    "Thus far, the approach used for predicting the genre, style and artist of an artwork is to train individual models for each attribute. However, there are clear correlations between certain classes among these labels, as seen in the data pre-processing stage. \n",
    "\n",
    "![](https://i.imgur.com/GYkjjy5.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Oh6iyvkpAL"
   },
   "source": [
    "To calculate possible correlations between the categorical data, we use the uncertainty coefficient, a measure of nominal association. \n",
    "\n",
    "For two discrete random variables $X$, $Y$, their Shannon entropies are given by $H(X) = -\\sum_x P_{X}(x)logP_{X}(x)$ and $H(Y) = -\\sum_y P_{Y}(y)logP_{Y}(y)$ respectively. \n",
    "\n",
    "The uncertainty coefficient is defined in the following manner:\n",
    "\n",
    "$U(X|Y)=\\frac{H(X)-H(X|Y)}{H(X)}$\n",
    "\n",
    "The uncertainty coefficient is in the range [0,1], where 0 denotes no association and 1 denotes full association. We use this over Cramer's V as it is asymmetric and thus more reflective of real-world relationships; feature A might give a lot of information about feature B, but the vice versa is not necessarily true. For instance, knowing that an artwork is by Monet implies that its style is likely impressionism, but an artwork's style being impressionism does not imply that the artist is Monet with an equally high probability.\n",
    "\n",
    "We make use of the [`dython`](https://github.com/shakedzy/dython/) package to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEkCDeQXlgZV"
   },
   "outputs": [],
   "source": [
    "#!pip list -v | grep \n",
    "\n",
    "# ensure that the following packages are installed with !pip list -v | grep <name>\n",
    "# you may have to restart the notebook to be able to use the package\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install seaborn\n",
    "#!pip install scipy\n",
    "#!pip install matplotlib\n",
    "#!pip install sklearn\n",
    "#!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3709,
     "status": "ok",
     "timestamp": 1651022450184,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "7LnFOEcXkSaV",
    "outputId": "b2f1f05b-bca4-468b-86b2-05bd4be0816d"
   },
   "outputs": [],
   "source": [
    "!pip install dython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2703,
     "status": "ok",
     "timestamp": 1651022515267,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "A68zwJLknDYV",
    "outputId": "79d9c425-5482-401f-a583-6b241a7643cd"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_Project/'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "\n",
    "current_folder = globals()['_dh'][0] + \"/\"\n",
    "try:\n",
    "  current_folder = path_to_file\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1271,
     "status": "ok",
     "timestamp": 1651022823321,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "mf4hAlGNjjGc",
    "outputId": "9d5e5bf3-9059-4ee4-b02e-20f32dc44d89"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'corr':                genre     style  artistName\n",
       " genre       1.000000  0.171027    0.252659\n",
       " style       0.167793  1.000000    0.781469\n",
       " artistName  0.216950  0.683955    1.000000,\n",
       " 'ax': <AxesSubplot:>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dython.nominal import associations\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(current_folder + 'augmented_data.csv')\n",
    "\n",
    "# delete irrelevant columns\n",
    "columns_removed = ['title', 'contentId', 'artistContentId', 'completitionYear', 'width', 'height']\n",
    "for column in columns_removed:\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "associations(df, nom_nom_assoc='theil', figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Two0t8zQr_h9"
   },
   "source": [
    "From the diagram, we see that the artist and style attributes are fairly closely associated. This association appears to be mutual, even if knowing an artwork's style yields more information about an artist's name than vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky11QZkysxBr"
   },
   "source": [
    "An architecture that accounts for these correlations would likely yield better predictive performance than the current 1 v REST approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwZ3tbkLs7rk"
   },
   "source": [
    "## CNN with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxEc04ygtJfk"
   },
   "source": [
    "The architecture of the network is updated from \n",
    "\n",
    "$L(54000,900) \\rightarrow ReLU \\rightarrow L(900,500) \\rightarrow ReLU \\rightarrow L(500,4) $\n",
    "\n",
    "to \n",
    "$L(54000,900) \\rightarrow ReLU  \\rightarrow DO(0.5) \\rightarrow L(900,500) \\rightarrow ReLU \\rightarrow L(500,4) $\n",
    "\n",
    "where \n",
    "- `L` is a linear layer with the specified input and output features\n",
    "- `ReLU` is the rectified linear activation function\n",
    "- `DO` is Dropout with the specified probability of dropping a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKjHZo--w5q4"
   },
   "source": [
    "We keep all other parameters and data constant.\n",
    "\n",
    "I.e. the optimizer is Adam, loss is given by the cross entropy loss, and training/testing sets are unchanged.\n",
    "\n",
    "*Note: the results of previous cells in this notebook are required to run the following cells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:54:49.655979Z",
     "iopub.status.busy": "2022-04-27T11:54:49.655436Z",
     "iopub.status.idle": "2022-04-27T11:54:49.668210Z",
     "shell.execute_reply": "2022-04-27T11:54:49.667503Z",
     "shell.execute_reply.started": "2022-04-27T11:54:49.655930Z"
    },
    "id": "1Zpw6vt5v67N"
   },
   "outputs": [],
   "source": [
    "class DropoutCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size, drop_prob=0.5):\n",
    "        super(DropoutCNN , self).__init__()\n",
    "\n",
    "        self.layer1  = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n",
    "        self.layer2  = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n",
    "        self.layer3  = nn.Linear(  hidden_size2 , output_size   , bias=False  ) \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        d       = self.dropout(y_hat)\n",
    "        z       = self.layer2(d)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:14:26.315954Z",
     "iopub.status.busy": "2022-04-27T12:14:26.315684Z",
     "iopub.status.idle": "2022-04-27T12:14:26.324867Z",
     "shell.execute_reply": "2022-04-27T12:14:26.324001Z",
     "shell.execute_reply.started": "2022-04-27T12:14:26.315923Z"
    },
    "id": "jFL8JFMpyPm3"
   },
   "outputs": [],
   "source": [
    "def get_test_loss(net, attribute, bs, criterion):\n",
    "\n",
    "    # choose appropriate test data\n",
    "    if attribute == 'genre':\n",
    "        test_data = X_test_genre\n",
    "        test_label = y_test_genre\n",
    "    elif attribute == 'style':\n",
    "        test_data = X_test_style\n",
    "        test_label = y_test_style\n",
    "    else:\n",
    "        test_data = X_test_artist\n",
    "        test_label = y_test_artist\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "    test_label = test_label.to(device)\n",
    "\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "\n",
    "    test_set_size = len(test_label)\n",
    "\n",
    "    net.eval()                                              \n",
    "    with torch.no_grad():     \n",
    "        for i in range(0, test_set_size, bs):\n",
    "\n",
    "            minibatch_data  =  test_data[i:i+bs]\n",
    "            minibatch_label =  test_label[i:i+bs]\n",
    "\n",
    "            actual_size = min(minibatch_data.shape[0], bs)\n",
    "\n",
    "            inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "            scores = net(inputs) \n",
    "\n",
    "            loss = criterion(scores, minibatch_label)\n",
    "\n",
    "            running_loss += loss.detach().item()\n",
    "\n",
    "            num_batches+=1\n",
    "\n",
    "\n",
    "    total_loss = running_loss/num_batches\n",
    "    print(\"Loss on test set: {:.2f}%\".format(total_loss))\n",
    "    \n",
    "    net.train()\n",
    "    return total_loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyit9PVlw91M"
   },
   "source": [
    "### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:19:22.305258Z",
     "iopub.status.busy": "2022-04-27T11:19:22.304900Z",
     "iopub.status.idle": "2022-04-27T11:19:22.349499Z",
     "shell.execute_reply": "2022-04-27T11:19:22.347098Z",
     "shell.execute_reply.started": "2022-04-27T11:19:22.305219Z"
    },
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1651026792296,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "cOjqOxeEw_v5",
    "outputId": "4a98b86c-2019-4a11-c722-71588c068490"
   },
   "outputs": [],
   "source": [
    "net_genre  = DropoutCNN(input_size,900,500,4)\n",
    "net_genre = net_genre.to(device)\n",
    "print(net_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U55QG-ngxTVT"
   },
   "outputs": [],
   "source": [
    "optimizer_genre = torch.optim.Adam(net_genre.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38977,
     "status": "error",
     "timestamp": 1651026844807,
     "user": {
      "displayName": "Oviya",
      "userId": "17385369900955446678"
     },
     "user_tz": -480
    },
    "id": "vbpUnw9uxrZL",
    "outputId": "495f9e63-63f5-4d5c-e809-299f570d8e60",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_genre = []\n",
    "error_per_epoch_genre = []\n",
    "test_error_per_epoch_genre = []\n",
    "test_loss_per_epoch_genre = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_genre = len(y_train_genre)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_genre)\n",
    " \n",
    "    for count in range(0, train_size_genre, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_genre.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_genre[indices]\n",
    "        minibatch_label =  y_train_genre[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_genre(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_genre.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_genre.append(eval_on_test_set(net_genre, 'genre', bs))\n",
    "    test_loss_per_epoch_genre.append(get_test_loss(net_genre, 'genre', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_genre.state_dict(), os.path.join(current_folder,'net_genre_dropoutcnn.pth'))\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.Tensor(loss_per_epoch_genre), os.path.join(current_folder,'loss_per_epoch_genre_dropout_cnn.pt'))\n",
    "torch.save(torch.Tensor(error_per_epoch_genre), os.path.join(current_folder,'error_per_epoch_genre_dropout_cnn.pt'))\n",
    "torch.save(torch.Tensor(test_error_per_epoch_genre), os.path.join(current_folder,'test_error_per_epoch_genre_dropout_cnn.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:12:10.170766Z",
     "iopub.status.busy": "2022-04-27T12:12:10.170055Z",
     "iopub.status.idle": "2022-04-27T12:12:10.311634Z",
     "shell.execute_reply": "2022-04-27T12:12:10.310467Z",
     "shell.execute_reply.started": "2022-04-27T12:12:10.170728Z"
    }
   },
   "outputs": [],
   "source": [
    "net_style = DropoutCNN(input_size,900,500,4)\n",
    "net_style = net_style.to(device)\n",
    "print(net_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:42:47.337535Z",
     "iopub.status.busy": "2022-04-27T11:42:47.337130Z",
     "iopub.status.idle": "2022-04-27T11:42:47.345572Z",
     "shell.execute_reply": "2022-04-27T11:42:47.344180Z",
     "shell.execute_reply.started": "2022-04-27T11:42:47.337493Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_style = torch.optim.Adam(net_style.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:42:54.070525Z",
     "iopub.status.busy": "2022-04-27T11:42:54.070248Z",
     "iopub.status.idle": "2022-04-27T11:45:33.341924Z",
     "shell.execute_reply": "2022-04-27T11:45:33.340347Z",
     "shell.execute_reply.started": "2022-04-27T11:42:54.070495Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_style = []\n",
    "error_per_epoch_style = []\n",
    "test_error_per_epoch_style = []\n",
    "test_loss_per_epoch_style = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_style = len(y_train_style)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_style)\n",
    " \n",
    "    for count in range(0, train_size_style, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_style.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_style[indices]\n",
    "        minibatch_label =  y_train_style[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_style(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_style.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_style.append(eval_on_test_set(net_style, 'style', bs))\n",
    "    test_loss_per_epoch_style.append(get_test_loss(net_style, 'style', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:46:57.856685Z",
     "iopub.status.busy": "2022-04-27T11:46:57.856395Z",
     "iopub.status.idle": "2022-04-27T11:46:58.600253Z",
     "shell.execute_reply": "2022-04-27T11:46:58.599196Z",
     "shell.execute_reply.started": "2022-04-27T11:46:57.856655Z"
    }
   },
   "outputs": [],
   "source": [
    "# change path as necessary for your environment\n",
    "torch.save(net_style.state_dict(), '/kaggle/working/net_style_dropoutcnn.pth')\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:47:07.259935Z",
     "iopub.status.busy": "2022-04-27T11:47:07.259605Z",
     "iopub.status.idle": "2022-04-27T11:47:07.267492Z",
     "shell.execute_reply": "2022-04-27T11:47:07.266441Z",
     "shell.execute_reply.started": "2022-04-27T11:47:07.259903Z"
    }
   },
   "outputs": [],
   "source": [
    "# change path as necessary for your environment\n",
    "\n",
    "torch.save(torch.Tensor(loss_per_epoch_style), '/kaggle/working/loss_per_epoch_style_dropout_cnn.pt')\n",
    "torch.save(torch.Tensor(error_per_epoch_style), '/kaggle/working/error_per_epoch_style_dropout_cnn.pt')\n",
    "torch.save(torch.Tensor(test_error_per_epoch_style), '/kaggle/working/test_error_per_epoch_style_dropout_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:55:03.854416Z",
     "iopub.status.busy": "2022-04-27T11:55:03.853596Z",
     "iopub.status.idle": "2022-04-27T11:55:07.021264Z",
     "shell.execute_reply": "2022-04-27T11:55:07.019540Z",
     "shell.execute_reply.started": "2022-04-27T11:55:03.854376Z"
    }
   },
   "outputs": [],
   "source": [
    "net_artist  = DropoutCNN(input_size,900,500,4)\n",
    "net_artist = net_artist.to(device)\n",
    "print(net_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:55:07.936013Z",
     "iopub.status.busy": "2022-04-27T11:55:07.935175Z",
     "iopub.status.idle": "2022-04-27T11:55:07.941360Z",
     "shell.execute_reply": "2022-04-27T11:55:07.940036Z",
     "shell.execute_reply.started": "2022-04-27T11:55:07.935952Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_artist = torch.optim.Adam(net_artist.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:55:35.699709Z",
     "iopub.status.busy": "2022-04-27T11:55:35.699264Z",
     "iopub.status.idle": "2022-04-27T11:58:22.217190Z",
     "shell.execute_reply": "2022-04-27T11:58:22.216482Z",
     "shell.execute_reply.started": "2022-04-27T11:55:35.699664Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_artist = []\n",
    "error_per_epoch_artist = []\n",
    "test_error_per_epoch_artist = []\n",
    "test_loss_per_epoch_artist = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_artist = len(y_train_artist)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_artist)\n",
    " \n",
    "    for count in range(0, train_size_artist, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_artist.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_artist[indices]\n",
    "        minibatch_label =  y_train_artist[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_artist(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_artist.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_artist.append(eval_on_test_set(net_artist, 'artist', bs))\n",
    "    test_loss_per_epoch_artist.append(get_test_loss(net_artist, 'artist', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:59:01.969895Z",
     "iopub.status.busy": "2022-04-27T11:59:01.969345Z",
     "iopub.status.idle": "2022-04-27T11:59:02.479705Z",
     "shell.execute_reply": "2022-04-27T11:59:02.478204Z",
     "shell.execute_reply.started": "2022-04-27T11:59:01.969857Z"
    }
   },
   "outputs": [],
   "source": [
    "# change path as necessary for your environment\n",
    "torch.save(net_artist.state_dict(), '/kaggle/working/net_artist_dropoutcnn.pth')\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:59:07.181922Z",
     "iopub.status.busy": "2022-04-27T11:59:07.181176Z",
     "iopub.status.idle": "2022-04-27T11:59:07.189000Z",
     "shell.execute_reply": "2022-04-27T11:59:07.188155Z",
     "shell.execute_reply.started": "2022-04-27T11:59:07.181882Z"
    }
   },
   "outputs": [],
   "source": [
    "# change path as necessary for your environment\n",
    "\n",
    "torch.save(torch.Tensor(loss_per_epoch_artist), '/kaggle/working/loss_per_epoch_artist_dropout_cnn.pt')\n",
    "torch.save(torch.Tensor(error_per_epoch_artist), '/kaggle/working/error_per_epoch_artist_dropout_cnn.pt')\n",
    "torch.save(torch.Tensor(test_error_per_epoch_artist), '/kaggle/working/test_error_per_epoch_artist_dropout_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the network is updated from \n",
    "\n",
    "$L(54000,900) \\rightarrow ReLU \\rightarrow L(900,500) \\rightarrow ReLU \\rightarrow L(500,4) $\n",
    "\n",
    "to \n",
    "$L(54000,900) \\rightarrow ReLU  \\rightarrow BatchNorm \\rightarrow L(900,500) \\rightarrow ReLU \\rightarrow L(500,4) $\n",
    "\n",
    "where \n",
    "- `L` is a linear layer with the specified input and output features\n",
    "- `ReLU` is the rectified linear activation function\n",
    "- `BatchNorm` is a layer of 1-dimensional batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep all other parameters and data constant.\n",
    "\n",
    "I.e. the optimizer is Adam, loss is given by the cross entropy loss, and training/testing sets are unchanged.\n",
    "\n",
    "*Note: the results of previous cells in this notebook are required to run the following cells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:12:28.163699Z",
     "iopub.status.busy": "2022-04-27T12:12:28.162706Z",
     "iopub.status.idle": "2022-04-27T12:12:28.171026Z",
     "shell.execute_reply": "2022-04-27T12:12:28.170094Z",
     "shell.execute_reply.started": "2022-04-27T12:12:28.163649Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchNormCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2,  output_size, drop_prob=0.5):\n",
    "        super(BatchNormCNN , self).__init__()\n",
    "\n",
    "        self.layer1    = nn.Linear(  input_size   , hidden_size1  , bias=False  )\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size1)\n",
    "        self.layer2    = nn.Linear(  hidden_size1 , hidden_size2  , bias=False  )\n",
    "        self.layer3    = nn.Linear(  hidden_size2 , output_size   , bias=False  ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y       = self.layer1(x)\n",
    "        y_hat   = torch.relu(y)\n",
    "        b       = self.batchnorm(y_hat)\n",
    "        z       = self.layer2(b)\n",
    "        z_hat   = torch.relu(z)\n",
    "        scores  = self.layer3(z_hat)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:19:36.755487Z",
     "iopub.status.busy": "2022-04-27T11:19:36.755226Z",
     "iopub.status.idle": "2022-04-27T11:19:40.121411Z",
     "shell.execute_reply": "2022-04-27T11:19:40.120561Z",
     "shell.execute_reply.started": "2022-04-27T11:19:36.755461Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_genre  = BatchNormCNN(input_size,900,500,4)\n",
    "net_genre = net_genre.to(device)\n",
    "print(net_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:19:40.268908Z",
     "iopub.status.busy": "2022-04-27T11:19:40.268247Z",
     "iopub.status.idle": "2022-04-27T11:19:40.273025Z",
     "shell.execute_reply": "2022-04-27T11:19:40.272277Z",
     "shell.execute_reply.started": "2022-04-27T11:19:40.268844Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_genre = torch.optim.Adam(net_genre.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:20:08.189695Z",
     "iopub.status.busy": "2022-04-27T11:20:08.189438Z",
     "iopub.status.idle": "2022-04-27T11:22:55.526887Z",
     "shell.execute_reply": "2022-04-27T11:22:55.525684Z",
     "shell.execute_reply.started": "2022-04-27T11:20:08.189668Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_genre = []\n",
    "error_per_epoch_genre = []\n",
    "test_error_per_epoch_genre = []\n",
    "test_loss_per_epoch_genre = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_genre = len(y_train_genre)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_genre)\n",
    " \n",
    "    for count in range(0, train_size_genre, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_genre.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_genre[indices]\n",
    "        minibatch_label =  y_train_genre[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_genre(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_genre.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_genre.append(eval_on_test_set(net_genre, 'genre', bs))\n",
    "    test_loss_per_epoch_genre.append(get_test_loss(net_genre, 'genre', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:29:47.475677Z",
     "iopub.status.busy": "2022-04-27T11:29:47.475414Z",
     "iopub.status.idle": "2022-04-27T11:29:48.139439Z",
     "shell.execute_reply": "2022-04-27T11:29:48.138558Z",
     "shell.execute_reply.started": "2022-04-27T11:29:47.475650Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(net_genre.state_dict(), '/kaggle/working/net_genre_batchnormcnn.pth')\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T11:29:51.324557Z",
     "iopub.status.busy": "2022-04-27T11:29:51.323731Z",
     "iopub.status.idle": "2022-04-27T11:29:51.331299Z",
     "shell.execute_reply": "2022-04-27T11:29:51.330595Z",
     "shell.execute_reply.started": "2022-04-27T11:29:51.324508Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(torch.Tensor(loss_per_epoch_genre), '/kaggle/working/loss_per_epoch_genre_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(error_per_epoch_genre), '/kaggle/working/error_per_epoch_genre_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(test_error_per_epoch_genre), '/kaggle/working/test_error_per_epoch_genre_batchnorm_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:12:52.662869Z",
     "iopub.status.busy": "2022-04-27T12:12:52.662581Z",
     "iopub.status.idle": "2022-04-27T12:12:55.884159Z",
     "shell.execute_reply": "2022-04-27T12:12:55.883288Z",
     "shell.execute_reply.started": "2022-04-27T12:12:52.662837Z"
    }
   },
   "outputs": [],
   "source": [
    "net_style  = BatchNormCNN(input_size,900,500,4)\n",
    "net_style = net_style.to(device)\n",
    "print(net_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:13:02.328059Z",
     "iopub.status.busy": "2022-04-27T12:13:02.327717Z",
     "iopub.status.idle": "2022-04-27T12:13:02.338472Z",
     "shell.execute_reply": "2022-04-27T12:13:02.337419Z",
     "shell.execute_reply.started": "2022-04-27T12:13:02.328020Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_style = torch.optim.Adam(net_style.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:14:47.386134Z",
     "iopub.status.busy": "2022-04-27T12:14:47.385387Z",
     "iopub.status.idle": "2022-04-27T12:17:39.114612Z",
     "shell.execute_reply": "2022-04-27T12:17:39.113841Z",
     "shell.execute_reply.started": "2022-04-27T12:14:47.386071Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_style = []\n",
    "error_per_epoch_style = []\n",
    "test_error_per_epoch_style = []\n",
    "test_loss_per_epoch_style = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_style = len(y_train_style)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_style)\n",
    " \n",
    "    for count in range(0, train_size_style, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_style.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_style[indices]\n",
    "        minibatch_label =  y_train_style[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_style(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_style.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_style.append(eval_on_test_set(net_style, 'style', bs))\n",
    "    test_loss_per_epoch_style.append(get_test_loss(net_style, 'style', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:18:12.662542Z",
     "iopub.status.busy": "2022-04-27T12:18:12.662225Z",
     "iopub.status.idle": "2022-04-27T12:18:13.198309Z",
     "shell.execute_reply": "2022-04-27T12:18:13.197436Z",
     "shell.execute_reply.started": "2022-04-27T12:18:12.662511Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(net_style.state_dict(), '/kaggle/working/net_style_batchnormcnn.pth')\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:18:23.906935Z",
     "iopub.status.busy": "2022-04-27T12:18:23.906680Z",
     "iopub.status.idle": "2022-04-27T12:18:23.914336Z",
     "shell.execute_reply": "2022-04-27T12:18:23.913308Z",
     "shell.execute_reply.started": "2022-04-27T12:18:23.906900Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(torch.Tensor(loss_per_epoch_style), '/kaggle/working/loss_per_epoch_style_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(error_per_epoch_style), '/kaggle/working/error_per_epoch_style_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(test_error_per_epoch_style), '/kaggle/working/test_error_per_epoch_style_batchnorm_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:02:31.296675Z",
     "iopub.status.busy": "2022-04-27T12:02:31.296074Z",
     "iopub.status.idle": "2022-04-27T12:02:31.869990Z",
     "shell.execute_reply": "2022-04-27T12:02:31.869269Z",
     "shell.execute_reply.started": "2022-04-27T12:02:31.296634Z"
    }
   },
   "outputs": [],
   "source": [
    "net_artist  = BatchNormCNN(input_size,900,500,4)\n",
    "net_artist = net_artist.to(device)\n",
    "print(net_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:02:43.178968Z",
     "iopub.status.busy": "2022-04-27T12:02:43.178695Z",
     "iopub.status.idle": "2022-04-27T12:02:43.183275Z",
     "shell.execute_reply": "2022-04-27T12:02:43.182332Z",
     "shell.execute_reply.started": "2022-04-27T12:02:43.178923Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_artist = torch.optim.Adam(net_artist.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:03:51.150247Z",
     "iopub.status.busy": "2022-04-27T12:03:51.149974Z",
     "iopub.status.idle": "2022-04-27T12:06:37.981333Z",
     "shell.execute_reply": "2022-04-27T12:06:37.980458Z",
     "shell.execute_reply.started": "2022-04-27T12:03:51.150217Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "loss_per_epoch_artist = []\n",
    "error_per_epoch_artist = []\n",
    "test_error_per_epoch_artist = []\n",
    "test_loss_per_epoch_artist = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    running_loss=0\n",
    "    running_error=0\n",
    "    num_batches=0\n",
    "\n",
    "    train_size_artist = len(y_train_artist)\n",
    "    \n",
    "    # obtain random indices\n",
    "    shuffled_indices = torch.randperm(train_size_artist)\n",
    " \n",
    "    for count in range(0, train_size_artist, bs):\n",
    "    \n",
    "        # set the gradients to zeros\n",
    "        optimizer_artist.zero_grad()\n",
    "        \n",
    "        # create a minibatch       \n",
    "        indices = shuffled_indices[count:count+bs]\n",
    "        minibatch_data  =  X_train_artist[indices]\n",
    "        minibatch_label =  y_train_artist[indices]\n",
    "        \n",
    "        # send them to the gpu\n",
    "        minibatch_data  = minibatch_data.to(device)\n",
    "        minibatch_label = minibatch_label.to(device)\n",
    "\n",
    "        actual_size = min(minibatch_data.shape[0], bs)\n",
    "        \n",
    "        # reshape the minibatch\n",
    "        inputs = minibatch_data.view(actual_size, 3*120*150)\n",
    "\n",
    "        # tell Pytorch to start tracking all operations that will be done on \"inputs\"\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # forward the minibatch through the net \n",
    "        scores = net_artist(inputs) \n",
    "\n",
    "        # Compute the average of the losses of the data points in the minibatch\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        \n",
    "        # backward pass   \n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent\n",
    "        optimizer_artist.step()\n",
    "        \n",
    "\n",
    "        # START COMPUTING STATS\n",
    "        \n",
    "        # add the loss of this batch to the running loss\n",
    "        running_loss += loss.detach().item()\n",
    "        \n",
    "        # compute the error made on this batch and add it to the running error       \n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "        running_error += error.item()\n",
    "        \n",
    "        num_batches += 1        \n",
    "    \n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss / num_batches\n",
    "    total_error = running_error / num_batches\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    loss_per_epoch_genre.append(total_loss)\n",
    "    error_per_epoch_genre.append(total_error)\n",
    "    \n",
    "    print('epoch=',epoch, '\\t time=', elapsed, '\\t loss=', total_loss , '\\t error=', total_error*100 ,'percent')\n",
    "    test_error_per_epoch_artist.append(eval_on_test_set(net_artist, 'artist', bs))\n",
    "    test_loss_per_epoch_artist.append(get_test_loss(net_artist, 'artist', bs, criterion))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:06:53.099984Z",
     "iopub.status.busy": "2022-04-27T12:06:53.099447Z",
     "iopub.status.idle": "2022-04-27T12:06:53.635401Z",
     "shell.execute_reply": "2022-04-27T12:06:53.634161Z",
     "shell.execute_reply.started": "2022-04-27T12:06:53.099945Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(net_artist.state_dict(), '/kaggle/working/net_artist_batchnormcnn.pth')\n",
    "\n",
    "# Then later:\n",
    "#model = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-27T12:06:55.873219Z",
     "iopub.status.busy": "2022-04-27T12:06:55.872809Z",
     "iopub.status.idle": "2022-04-27T12:06:55.883187Z",
     "shell.execute_reply": "2022-04-27T12:06:55.882427Z",
     "shell.execute_reply.started": "2022-04-27T12:06:55.873185Z"
    }
   },
   "outputs": [],
   "source": [
    "# modify path as necessary for your chosen environment\n",
    "torch.save(torch.Tensor(loss_per_epoch_artist), '/kaggle/working/loss_per_epoch_artist_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(error_per_epoch_artist), '/kaggle/working/error_per_epoch_artist_batchnorm_cnn.pt')\n",
    "torch.save(torch.Tensor(test_error_per_epoch_artist), '/kaggle/working/test_error_per_epoch_artist_batchnorm_cnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Dropout & Batch-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier sections, we applied dropout and batch regularization to vanilla CNN classifiers, with all other parameters being kept constant. In this section, we evaluate the performance of the resulting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tabulated accuracy scores of the models on the test sets are as follows.\n",
    "\n",
    "| Model\\ Label | Genre | Style | Artist |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| MLP | 70.99% | 64.39% | 67.37% |\n",
    "| Vanilla CNN | 71.04% | 66.90% | 56.81 |\n",
    "| Dropout CNN | 49.30% | 52.18% | 39.42% |\n",
    "| Batch-normalized CNN | 59.45% | 60.12% | 57.27% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4624, 0.4776, 0.5397, 0.5520, 0.5468, 0.5273, 0.5182, 0.5218, 0.5204,\n",
      "        0.4810, 0.4826, 0.5142, 0.5237, 0.5048, 0.4991, 0.5046, 0.5064, 0.5089,\n",
      "        0.5015, 0.5127, 0.4934, 0.4944, 0.5041, 0.4869, 0.4980, 0.4917, 0.5059,\n",
      "        0.4944, 0.5019, 0.4953, 0.5000, 0.4941, 0.5012, 0.4963, 0.5032, 0.5085,\n",
      "        0.5046, 0.5039, 0.4921, 0.5089, 0.5021, 0.5145, 0.5132, 0.5092, 0.5031,\n",
      "        0.5147, 0.5089, 0.5051, 0.4901, 0.5070])\n",
      "tensor([0.3372, 0.3471, 0.3217, 0.3202, 0.3104, 0.3242, 0.3190, 0.3247, 0.3334,\n",
      "        0.3114, 0.3368, 0.3288, 0.3288, 0.3123, 0.3470, 0.3381, 0.3396, 0.3334,\n",
      "        0.3365, 0.3545, 0.3225, 0.3292, 0.3746, 0.3440, 0.3390, 0.3482, 0.3566,\n",
      "        0.3469, 0.3469, 0.3560, 0.3602, 0.3603, 0.3577, 0.3557, 0.3690, 0.3571,\n",
      "        0.3522, 0.3830, 0.3710, 0.3550, 0.3631, 0.3792, 0.3592, 0.3665, 0.3799,\n",
      "        0.3830, 0.3710, 0.3599, 0.3596, 0.4055])\n"
     ]
    }
   ],
   "source": [
    "test_error_per_epoch_genre_dropout = torch.load(os.path.join(current_folder,'test_error_per_epoch_genre_dropout_cnn.pt'))\n",
    "print(test_error_per_epoch_genre_dropout)\n",
    "\n",
    "test_error_per_epoch_genre_batchnorm = torch.load(os.path.join(current_folder,'test_error_per_epoch_genre_batchnorm_cnn.pt'))\n",
    "print(test_error_per_epoch_genre_batchnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4638, 0.5513, 0.5108, 0.5032, 0.5291, 0.5340, 0.5195, 0.5040, 0.5297,\n",
      "        0.5552, 0.5535, 0.5512, 0.5808, 0.5191, 0.4944, 0.5061, 0.5338, 0.4840,\n",
      "        0.5315, 0.4831, 0.5166, 0.4820, 0.5059, 0.5113, 0.5127, 0.4949, 0.4953,\n",
      "        0.4858, 0.5017, 0.5280, 0.5298, 0.5290, 0.4943, 0.5013, 0.5109, 0.4875,\n",
      "        0.5286, 0.4975, 0.4913, 0.4850, 0.5098, 0.5244, 0.4670, 0.5093, 0.5025,\n",
      "        0.4769, 0.4967, 0.4653, 0.4816, 0.4782])\n",
      "tensor([0.3680, 0.3506, 0.3471, 0.3489, 0.3438, 0.3532, 0.3441, 0.3509, 0.3456,\n",
      "        0.3479, 0.3554, 0.3599, 0.3517, 0.3395, 0.3778, 0.3440, 0.3477, 0.3856,\n",
      "        0.3663, 0.3589, 0.3607, 0.3809, 0.3690, 0.3732, 0.3702, 0.3949, 0.3853,\n",
      "        0.3836, 0.3747, 0.3758, 0.3895, 0.3895, 0.4065, 0.3990, 0.3958, 0.4123,\n",
      "        0.3940, 0.4325, 0.3831, 0.3721, 0.4189, 0.3740, 0.3794, 0.3925, 0.3895,\n",
      "        0.3830, 0.3857, 0.3857, 0.4131, 0.3988])\n"
     ]
    }
   ],
   "source": [
    "test_error_per_epoch_style_dropout = torch.load(os.path.join(current_folder,'test_error_per_epoch_style_dropout_cnn.pt'))\n",
    "print(test_error_per_epoch_style_dropout)\n",
    "\n",
    "test_error_per_epoch_style_batchnorm = torch.load(os.path.join(current_folder,'test_error_per_epoch_style_batchnorm_cnn.pt'))\n",
    "print(test_error_per_epoch_style_batchnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6238, 0.6459, 0.6057, 0.6374, 0.6275, 0.5833, 0.6173, 0.6086, 0.6211,\n",
      "        0.6114, 0.5905, 0.6175, 0.6070, 0.5858, 0.6056, 0.6093, 0.6143, 0.6201,\n",
      "        0.5806, 0.5860, 0.5661, 0.5882, 0.5792, 0.6113, 0.5689, 0.6443, 0.5730,\n",
      "        0.5884, 0.5831, 0.5809, 0.5808, 0.5936, 0.5964, 0.5875, 0.5865, 0.5703,\n",
      "        0.5907, 0.5814, 0.6100, 0.5719, 0.5877, 0.5799, 0.5834, 0.5965, 0.5843,\n",
      "        0.5882, 0.5766, 0.5955, 0.5915, 0.6058])\n",
      "tensor([0.4080, 0.4095, 0.4037, 0.3868, 0.3831, 0.4081, 0.3791, 0.3816, 0.3830,\n",
      "        0.3710, 0.3981, 0.3804, 0.3711, 0.3819, 0.3722, 0.3823, 0.3673, 0.3977,\n",
      "        0.3801, 0.4058, 0.4019, 0.4158, 0.3964, 0.3977, 0.4062, 0.4110, 0.4211,\n",
      "        0.4243, 0.4044, 0.3931, 0.4235, 0.4209, 0.4196, 0.4431, 0.4219, 0.4270,\n",
      "        0.4193, 0.4503, 0.3992, 0.4272, 0.4189, 0.4169, 0.4421, 0.4121, 0.4217,\n",
      "        0.4291, 0.4368, 0.4508, 0.4189, 0.4273])\n"
     ]
    }
   ],
   "source": [
    "test_error_per_epoch_artist_dropout = torch.load(os.path.join(current_folder,'test_error_per_epoch_artist_dropout_cnn.pt'))\n",
    "print(test_error_per_epoch_artist_dropout)\n",
    "\n",
    "test_error_per_epoch_artist_batchnorm = torch.load(os.path.join(current_folder,'test_error_per_epoch_artist_batchnorm_cnn.pt'))\n",
    "print(test_error_per_epoch_artist_batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout model was underfitted, and more data would likely help with the training of the model.\n",
    "\n",
    "There are clear signs of overfitting on the dropout and batch-normalized models. For instance, at epoch 50, the batch-normalized artist classifier has a training error rate of 5.83% and a testing error rate of 42.73%. Similar patterns were observed for genre and style for both models. \n",
    "\n",
    "The batch-normalized model appeared to converge faster than the vanilla cnn, with overfitting occuring after the 16th epoch for the style and artist classifiers, and after the 4th epoch for the genre classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel classification architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we utilized a one vs rest approach for multilabel classification of artworks using binary relevance. \n",
    "\n",
    "Future work can be done on training classifiers that account for this association, with a focus on style and artist name - as those were identified to be the two closely associated variables by the uncertainty coefficient metric in the earlier analysis. One possible way of achieving this is to construct a classifier chain. The prediction of a binary classifier for style can be used as input to train a secondary classifier for artist name. \n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*ycwr_uE8_5lnOMNCnFOuXQ.png)\n",
    "*Example of a classifier chain structure with three labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger dataset, More classes for each label, More labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project was constructed to maximize the initial data available for training before augmentation. Future work can be on classifying artwork among similar genre, style or other labels. This might pose an issue with the amount of data available in typical data science tasks, and data augmentation will be a more significant step of such a project. \n",
    "\n",
    "Other labels such as the time period of an artwork's completion, and emotion detection can be explored as well, with the latter having possible applications to other fields such as robotics. \n",
    "\n",
    "Digital art is another avenue for exploration, and such a classifier would be useful in the detection of use of images without proper attribution to the artist on various platforms in the internet, with a possible application to non-fungible tokens on platforms like DeviantArt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we modelled the labelling of artwork with MLPs and CNNs, and investigated the effects of dropout and batch-normalization on the performance of a model. Batch-normalization converged faster than vanilla CNNs, but exhibited signs of overfitting, which early stopping can improve. Dropout led to a loss in performance, but prevented overfitting. Dropout coupled with a larger dataset to train on is a possible extension to this project, along with exploring the connotations of associations between categorical variables, as in the case of this problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
